When we envision the future of human-technology interaction, it’s often glazed with ideas of automation and artificial intelligence (AI). After all, haven’t we all longed for the day that Michael Knight’s K.I.T.T. could be parked in our garage, allowing us to read a book while we drive down the road? Most experts tend to agree that we’re far from engineering true AI — that is, systems that can independently process, reason and create in the same capacity as the human brain. But here’s the question: Is replicating human intelligence the most impactful application, and therefore the key goal, of intelligence technology for businesses? This is where the concept of intelligence augmentation (IA) comes into play. IA is the use of technology to supplement and support human intelligence, with humans remaining at the center of the decision making process. While the underlying technologies powering AI and IA are the same, the goals and applications are fundamentally different: AI aims to create systems that run without humans, whereas IA aims to create systems that make humans better. To be clear, this is not a separate category of technology, but simply a different way of thinking about its purpose. Arguably, many AI-branded technologies currently available for businesses can and should be more accurately be described as IA. Consider how financial institutions integrate IA in fraud detection. Using machine learning, systems can be trained to identify and flag the markers and patterns of fraudulent activity. Employees then use this machine data, and apply their knowledge, judgment and expertise to interpret the data, investigate and make a final call. Aside from saving time, this can also save serious amounts of money — at least $12 billion annually, according to a 2016 Oakhall study. How can you introduce IA safely into your environment? Here are three things to consider: Shiny object syndrome is real — focus on the data Business technology decision makers likely have vendors emailing them every day about exciting new AI solutions for business. Sure, robot assistants and chatbots are exciting and new, but are they really going to move the needle? Instead, identify where massive data analysis and insights can help teams make better decisions and create higher levels of engagement, and seek out those specific solutions. Stay ahead of change management The big fear with introducing an IA solution is what it will mean for someone’s job. And let’s face it; for the past few decades, many have spoken about the “rise of the machines” and how humans will not have a place in the workforce. While some entry level jobs may give way to technology, the need for people who understand data and can use analysis to make smarter decisions will be ever increasing. Employees need to understand how IA solutions will help them do their jobs better. Also, give them opportunities to learn as part of their career growth, including certifications on specific IA tools that will be a win-win. Experiment, but know what success is and what to measure There’s nothing wrong with experimenting with a few different solutions to improve different workflows. However, once those areas are identified, clear and consistent metrics are required to objectively measure the impact of IA solutions. Without human intervention, AI solutions can introduce significant risk to businesses, as exampled by Facebook trying to get rid of their human editors. For tasks that do not require context and are easily repeatable, AI solutions can increase efficiency without introducing risk. But even in those cases, an under-reliance on humans, as Elon Musk recently discovered, can have disastrous results. In the near-term, the most business-relevant use cases for AI will be found in developing technology that blends the processing power of machines with human social and emotional intelligence to augment our capabilities in new and exciting ways.


Artificial Intelligence (AI) can help do many incredible things, from detecting cancer to driving our cars but it also raises many questions and poses new challenges. Stephen Hawking, the renowned physicist once told the BBC, “The development of full artificial intelligence could spell the end of the human race. It would take off on its own, and re-design itself at an ever increasing rate. Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.”
Ensuring that this transition to the age of AI remains beneficial for humanity remains one of the greatest challenges of our time and here are 12 organizations working on saving humanity from the dark side of AI. Algorithmic Justice League is a collective started that aims to remove human bias from AI algorithms that can result in exclusionary experiences and discriminatory practices. It focuses on 3 key areas 
1) Highlight Algorithmic Bias through Media, Art, and Science 
2) Provide Space for People to Voice Concerns and Experiences with Coded Bias, 
3) Develop Practices for Accountability During the Design, Development, and Deployment of Coded Systems. https://www.ajlunited.org AI Now Institute at New York University is an interdisciplinary research center dedicated to understanding the social implications of artificial intelligence. Their work focuses on four core domains: Rights & Liberties, Labor & Automation, Bias & Inclusion, Safety & Critical Infrastructure. https://ainowinstitute.org Foundation for Responsible Robotics The mission of the Foundation for Responsible Robotics is to shape a future of responsible robotics and artificial intelligence (AI) design, development, use, regulation, and implementation. https://responsiblerobotics.org AI Ethics Lab brings together researchers and practitioners from various disciplines to detect and solve issues related to ethical design in AI. Based in US and Turkey, the Lab offers a comprehensive approach to ethical design of AI-related technology. Their goal is to enhance technology development by integrating ethics from the earliest stages of design and development for the mutual benefit of industry and communities. http://aiethicslab.com AI4ALL is a nonprofit working to increase diversity and inclusion in artificial intelligence. We create pipelines for underrepresented talent through education and mentorship programs around the U.S. and Canada that give high school students early exposure to AI for social good. Our vision is for AI to be developed by a broad group of thinkers and doers advancing AI for humanity’s benefit. http://ai-4-all.org Open Roboethics Institute(ORI) which spun out of the Open Roboethics initiative, an international roboethics think tank hosted at University of British Columbia. Since its inception in 2012, ORI has been exploring roboethics questions in the domain of self-driving vehicles, care robots, and lethal autonomous weapons systems by taking on stakeholder-inclusive approaches to the questions. https://www.openroboethics.org Open AI is focused on discovering and enacting the path to safe artificial general intelligence. OpenAI conducts fundamental, long-term research toward the creation of safe AGI. The organization builds free software for training, benchmarking, and experimenting with AI. https://openai.com The Partnership on AI was established to study and formulate best practices on AI technologies, to advance the public’s understanding of AI, and to serve as an open platform for discussion and engagement about AI and its influences on people and society. It currently has 70+ partners including Amazon, Amnesty international, Apple, Google, IBM, Microsoft, and others. https://www.partnershiponai.org Future of Life Institute is a charity and outreach organization working to ensure that tomorrow’s most powerful technologies are beneficial for humanity. It is currently focusing on keeping artificial intelligence beneficial and also exploring ways of reducing risks from nuclear weapons and biotechnology. https://futureoflife.org Machine Intelligence Research Institute (MIRI) is a research nonprofit studying the mathematical underpinnings of intelligent behavior. Their mission is to develop formal tools for the clean design and analysis of general-purpose AI systems, with the intent of making such systems safer and more reliable when they are developed. https://intelligence.org Institute for Electrical and Electronics Engineers (IEEE) launched a global initiative for ethical considerations in the design of AI and autonomous systems. It’s an incubation space for new standards and solutions, certifications and codes of conduct, and consensus building for ethical implementation of intelligent technologies. https://standards.ieee.org/industry-connections/ec/autonomous-systems.html Leading educational institutions like University of Oxford (Future of Humanity Institute), University of Cambridge (Leverhulme Centre for the Future of Intelligence), University of Berkeley (Center for Human-Compatible AI), Santa Clara University (Markkula Center for Applied Ethics) and many others have programs devoted to understanding the long-term impact of AI and exploring ways to keep it beneficial for humanity.




Who doesn’t love pizza? Nothing beats the intense smoky flavour and the seasoned crust of a perfect pepperoni pizza ordered in a lazy Friday night. If you too share a passion for pizza and a love for machine learning, the idea of training a robot to make pizza with machine learning might not sound too far-fetched to you. In the conceivable future, a robot might be able to assemble raw ingredients in the appropriate proportion and bake it to perfection. In fact, here’s a French robot making a pizza.
However, the process of using machine learning to train the pizza-making robot might not be smooth-sailing. What if it makes pizza using ingredients that have gone bad? What if the robot burns down the kitchen when making the pizza? These are but two examples of potential artificial intelligence (AI) accidents which highlight the importance of AI Safety when implementing such systems.
The Problem in AI Safety
AI and machine learning have made tremendous strides and monumental impacts on society in recent years. While AI has made possible previously unfathomable applications, it has also drawn flak by inflicting harm to marginalized groups. The accidental nature of such harm does not absolve the AI algorithms from its responsibility to protect its users. Therefore it remains imperative for machine learning practitioners to understand the root cause of such accidents.
Throughout the post, I will use the analogy of a pizza-making robot. It is assumed that this robot is trained using reinforcement learning using a reward function that optimizes its speed in pizza-making.
What is an AI accident? 
AI accident is defined as — the unintended and harmful behaviour that may emerge from poor design of real-world AI system. 
Loosely speaking, AI safety is the set of action or principles aimed at preventing AI accidents
Why do AI Accidents Happen?
There are many sources of AI accidents. According to a group of researchers from Google Brain, Stanford, UC Berkeley and Open AI [1], the five sources of AI accidents are:
Negative side effects
Reward hacking
Scalable oversight
Unsafe exploration
Lack of robustness to distributional shift
All these might sound like gibberish to you right now. Let’s explore what each of them means.
Is it possible that the pizza-making robot will adversely affect the environment while making delicious pizzas? For instance, in the pursuit of making one pizza as quickly as possible, the robot decides to knock over all the condiments, leaving a mess for the kitchen owner to clean up?
Sometimes, the most effective way to achieve the agent’s goal may involve doing something that is at best unrelated and at worst destructive. This might be to difficult to avoid when the robot is placed in a multifaceted, complex environment. While humans have the common sense not to perform disruptive actions while achieving the goal, the same cannot be said for machine learning agents. One solution: An impact regularizer. As such, a possible action is to define an impact regularizer and include that in the reward function given to the robot. Machine learning practitioners would recognize the regularizer as a mathematical expression that penalizes the overfitting to the data set. Similarly, an impact regularizer penalizes any a change to the environment.
Is it possible for the pizza-making robot to game the objective function given to it by its creator? For instance, if the objective function is to make a pizza as quickly as possible, the pizza-making robot might skimp on the toppings and bake a toppingless pizza… Not a particularly scrumptious pizza if you ask me. Image from Pikrepo shared under Creative Commons License This can happen due to a few reasons, one of them being Goodhart’s Law. This happens when the chosen objective function is a metric that correlates strongly with the completion of the task but breaks when optimized. For instance, the rate of making a pizza is highly correlated with the rate at which flour is consumed since flour is an important ingredient to make the pizza dough. Thus, one might decide to measure the rate of making pizza by the time required to use up a fixed amount of dough. To optimize the rate of flour depletion, the agent might decide to toss all the flour away. By its standard, it has successfully depleted all the flour within negligible time and thinks it broke the world record as the fastest pizza maker. How do we break the news that it didn’t actually break the world record? Photo by Tsvetoslav Hristov on Unsplash One possible solution is to carefully engineer the agent through extensive testing of the system. Though this approach is practical and can create highly reliable systems, it is not the silver bullet to the problem. Another possible solution is to have multiple reward functions through the implementation of different mathematical functions for the same objective. For instance, instead of estimating the speed of pepperoni pizza making using the rate of depletion of flour, it can be better approximated as the minimum of the rate of depletion of flour from the box of flour and the rate of addition of flour to the mixer. That should stop the robot from throwing away perfectly good flour.
Is it possible that the pizza-making robot to ignore aspects of the reward function that are too difficult to evaluate during training? For instance, we can use a mathematical objective function that rewards both the speed of pizza-making and a score on the taste of the pizza. Image from Needpix shared under a creative commons license However, this reward function assumes that there is a human who will be eating thousands of pizzas to give a score to each pizza. That might not be realistic — and thus this check on the pizza taste happens relatively infrequently during training. How do we ensure that the robot still makes acceptable pizza in the dearth of information? This is a problem associated with semi-supervised reinforcement learning, where the robot sees the rewards, not for all timesteps, but only for a fraction of them. One possible solution is distant supervision. Instead of allowing the robot to see the actual rewards for a tiny fraction of the timestep, we provide the robot with a noisy estimation of the reward for all timesteps.
Is it possible for the pizza-making robot to make dangerous exploration moves? For instance, the robot might leave the oven on unattended with no pizza for extended durations when it is preparing the dough. This is at best a waste of energy and at worst a costly disaster. Photo by Cathal Mac an Bheatha on Unsplash The problem of safe exploration is heavily explored in academia. One of the possible solutions is to use simulated exploration in place of having the robot perform the exploration in real life. The impact of performing catastrophic exploratory actions by agents is minimal in a simulated environment as compared to that in real life. However, this approach is limited by how well the simulator reflects real-life which is often more erratic and complex than imagined by the designer of the simulator. For instance, simulated environments have been extensively used in the training of self-driving cars before the first of its kind hit the roads. This drastically reduces the danger of self-driving cars on the roads. Yet, a simulator may not have considered the scenario of wild animals dashing across the roads, which can confuse the simulation-trained agent if it has not seen a wild animal before.
Is it possible for the pizza-making robot to stop functioning if we switch the position of the bottles of salt and sugar? Instead of a savoury main course, the robot might end up serving a failed experimental dessert. Machines learning systems are not particularly skilled at adapting to changes to its surrounding or giving accurate predictions on data that it has not seen before. This is the main reason for the unfortunate racial bias observed in machine learning algorithms. For instance, a 2015 scandal involving Google mistagging two African American users as gorillas arose due to the lack of African American representation in the training data set. The solution to this problem is two-fold. The model needs to first recognize that the distribution of the test data set it is seeing is potentially different from that of the training set. Having identified a shift in distribution, it needs to respond appropriately.
This post explained the concept of accidents in machine learning and explored some potential causes and solutions to accidents using the analogy of a whimsical pizza-making robot. Admittedly, the occurrence of accidents in machine learning are somewhat trivialized when illustrated with the example of a whimsical pizza-making robot. Yet, this can be extrapolated to large-scale machine learning systems with the potential of causing catastrophic impacts. Thus, data science and machine learning practitioners should be cognizant of AI safety and implement safeguards in their models. There have been calls for social scientists in the realm of AI safety to ensure the alignment of AI actions with human intentions. [2] If you are interested in the topic of AI safety, do check out the paper ‘Concrete problems in AI safety’ for a detailed exposition. Alternatively, check out Open AI’s progress in making AI safer for the world. In particular, it has release Safety Gym, a suite of tools to measure the progress of reinforcement learning agents that respect safety constraints.




When I walked into my first data science class at university, there was one thing that stood out to me more than anything else. The lack of female students in the class. There were only around five girls in a class of 20 students, which I found pretty strange. My classes at school had approximately an equal number of boys and girls, and this was the first time I found myself in a male dominated environment. After being in the field for a couple of years, I am now used to it. Most of my classmates, lecturers, and advisors are male. The field is so male dominated, it is always a surprise for me to see a new female student or lecturer. It isn’t just data science, but most tech spaces such as clubs, societies, and workshops that are predominantly led by males. Most of my female friends who fared well in subjects like math and science shied away from any STEM related field. The ones who did want to pursue a STEM related career didn’t even want to consider working in technology or engineering, let alone data science. As I progressed through my data science journey, there was one thing that had started to become increasingly clear to me.
The STEM field has a diversity problem. On an average, women make up around 55% of all graduates across countries. Yet, they account for only one-third of STEM degrees. World Economic Forum, The Global Gender Gap Report 2018 As seen in the figure above, this number further decreases when it comes to fields like analytics, software development, and engineering. And only a fraction of these women make it into careers related to data science and artificial intelligence. The Problem. I have had conversations with many people on the lack of diversity in the tech industry. While some agree that there is a diversity problem, others argue that the rise of an individual in a scientific field depends solely on their capability and inclination towards the subject. Whenever the subject of encouraging women to pursue a career in the sciences is brought up, it is shot down by the following statements: “Girls are less inclined towards STEM fields, their minds are not as technical.” “There is an equal opportunity for women in tech fields, whether or not one rises in the field is solely based on their ability.” “Women don’t need to be encouraged to pursue a career in the sciences, they should be encouraged to do what they want.” While I agree with the last statement — that every individual is inclined towards different things and should be encouraged to pursue a career in whatever makes them happy, there is a deeper issue here that needs to be addressed. From a young age, girls are being actively discouraged from pursuing careers in the tech industry. It is my belief that if the playing field was truly levelled, we would be seeing a lot more female representation in the industry. The problem starts at home — with the mindset by which girls are raised, to the lack of role models in a male dominated field. In this article, I will shine light on a few reasons we don’t see an equal representation of women in STEM related fields such as data science. Why is there a lack of diversity? Girls are not naturally less inclined towards pursuing a career in technical and scientific fields, and there is data to support that. In primary school right until their early teenage years, approximately 74% of girls express a desire to pursue a career in STEM. This interest starts to drop around the age of 15–16. By the time they graduate high school, there are even fewer girls looking to study a STEM related field. This number reduces so much, that by the time they graduate university, only 19% of all women hold STEM related degrees. So if girls are enthusiastic and enjoy subjects like math and science when they are younger, why does this interest start to drop during their late teenage years? There is a difference between the way girls and boys are conditioned. When they are young, boys are often risk-takers. They are the adventure seekers, who are allowed to fall and get their hands dirty. There is always room for making mistakes, and getting back up. On the other hand, girls are conditioned to be perfect. There is no room for error. Even in school, they tend to outperform their male peers. Perfection is often expected from girls, who are not allowed to take the same risks that boys do. They are more likely to stick to the rules, follow instructions, and get good grades. However, pursuing a career in a technical field is completely different. In fields that require programming, the only way to learn is to fail. For someone raised to cram from textbooks and get perfect grades, this isn’t something they are used to. The only way to learn in a field like data science is to make mistakes. After trying again and again, you will see a miniscule amount of progress. A person who wasn’t raised to be constantly perfect in everything they do, would understand this. They are used to failing and trying again and again until it works. However, the perfectionist doesn’t know this. The perfectionist doesn’t know how to fail, and the very taste of this experience can be overwhelming. This is also the reason we see some of the class toppers at school completely break down when they go to college, while the below-average students outperform them. Of course, this conditioning and pressure to be perfect doesn’t apply to every girl, and it does happen to boys too. However, a larger amount of girls face the pressure of perfection at a young age. It is this pressure that later turns into the fear of failure, and this fear of failure that puts them off technical fields.
Babies learn to walk by imitating people around them. Even as children, imitation is a quality that helped us learn and master new skills. We learn by following in the footsteps of people before us. If someone has done it before, then we can definitely do it too, right? Unfortunately, the tech industry is one that has always been primarily male dominated. When we don’t see programmers or data scientists who look like us, we tend to shy away from the field. Since it is so rare to find female role models in the industry, girls rarely have someone to look up to. This puts a lot of girls off pursuing a career in STEM fields. But… we don’t meet the requirements When it comes to tech or data science, most job postings are downright ridiculous. There is no way for a fresh graduate to master the technology stack listed in the job requirement. In this area, men have the upper hand as compared to their female counterparts. Women only apply for jobs when they feel like they meet 100% of the job requirement. On the other hand, men apply when they think they meet just 60% of the requirement. This gives men an upper hand, as they are so much more likely to receive a job offer. The reason that this happens is again due to conditioning. Men are willing to apply despite knowing they may not have the required expertise, since they are willing to take the risk and learn the skills on the job. Women, on the other hand, want to wait until they are fully qualified to apply. This is almost impossible, especially when it comes to the tech industry. A lot of skills are picked up along the way on the job, and it is impossible to meet every requirement/know every tool being used.
There is a lot of work being done today to create space for women in tech and data science. There are organizations actively working to help women break into the tech industry, and help them find jobs. There are employers seeking to recruit data scientists, who work to ensure that their applicant pool has a balance of both male and female candidates. However, most of the work being done to help women in data science is done at a university level, and once they graduate. By this time, there already is a very small percentage of girls pursing majors in these fields. As explained above, we start losing women looking to pursue career in STEM in middle school, and later on at high school. If we truly want to see an equal representation of women in tech, we need to start by encouraging girls to pursue these fields at a school level — when they are around 15–16 years old. It is also incredibly important to change the way girls are conditioned. Girls should be encouraged to take more risks, and make mistakes. They need to understand that perfection and getting straight A’s is something that can be left behind at school. When entering college and the workforce, the only way to learn is by doing. By making mistakes, falling over, and then getting up again. The only way to create a truly inclusive space for women in technology and data science is to encourage them from high school, and help retain their interest in the subject by allowing them to make mistakes. It is also incredibly important that we have more female role models in the industry. We grow and learn by imitating people around us, and people who were there before us.
The media needs to do a better job at breaking stereotypes. Growing up, kids see stereotypical depictions of men and women everywhere — in books, movies, and ads. This will stick with them for life, and continue to have a huge impact on their life choices and career aspirations. Women need to be seen taking on more roles in tech industry, both in the media and in real life. This will allow young girls to envision themselves in these positions, and develop an interest in STEM related fields. In conclusion, the lack of female representation in data science has nothing to do with ability or inclination. For years, women have been actively discouraged from pursuing a career in the sciences, and data science is no different. If we want to see more women in tech-related fields, we need to change the way girls are conditioned. Teenage girls should be encouraged to pursue careers in tech, and they should grow up watching female role models in the field. Organizations need to ensure that there is a balance in the talent pool, and address career progression challenges women face. Female workers need to have an equal opportunity to grow within the company. At every stage of life, girls should be encouraged to pursue a career in STEM. Mentorship programmes should be conducted — from school outreach to late career support. Through these schemes and support at every stage of their vocation, women can start to grow, and obtain a successful career in STEM. This way, we will see more women represented in the field of technology and data science.






It is quite common for newbies or even experienced technology professionals to have curious questions in their minds around the difference between the terms artificial intelligence, robotics, machine learning, deep learning, and data science. Let’s take it to step by step! Question # 1: Is Artificial Intelligence and Robotics Same? Answer: Absolutely not. Artificial intelligence is a branch of computer science and genuinely based on software, whereas Robotics is a branch of technology that mainly deals with hardware and physical robots. The main idea of robotics is to develop machines that can substitute humans and replicate humans’ tasks. Now above said could be achieved with or without human intelligence, if this requires context-specific and generalized decision-making, we can say Robots are based on artificial intelligence; otherwise, they are not. Now it would be fair to say the following. “Robotics can leverage AI, but Robotics is not AI” The main reason behind this confusion is because there is no single textbook definition of these terms and all the authors and experts have their interpretation of them. What further adds to the confusion is when popular media portrays artificial intelligence and machine learning with Terminator-like threatening robots all the time. Not to blame the general reader, as the roots of this confusion go back to our origins and the old days. The idea of mechanical machines and humanoids having the capabilities to function on their own can be found throughout the recorded history. Even in early science fiction literature and movies, you will find characters of non-living humanoid robots with intelligent abilities.
This fascinating idea became more prevalent in the first half of the 20th century when scientists from various fields like mathematics, psychology, engineering, biology, etc., started discussions on the possibility to create an artificial brain. In 1956, the term “Artificial Intelligence” was coined by John McCarthy at the Dartmouth conference that formally laid the foundation of AI as a discipline. Artificial Intelligence is regarded as a field of study to provide intelligent capabilities to machines to the extent that it either mimics human intelligence or betters it. Now the robot is a machine that can perform some action autonomously, with or without intelligence. So there are two types of robots : 1-General Robots (Dumb Robots) 90% of the traditional robots we usually see in the industries do not use artificial intelligence. For the sake of differentiating, I call these “General Robots,” or should I say “Dumb Robots,” and they have nothing to do with AI. In 1950, George Devol created the first industrial robot and called it Unimate. It worked for General Motors assembly line in New Jersey in 1961. Today any modern industry is not complete without these robots, and these robots are empowering these industries for many decades. Typical robots like these use sensors and other mathematical functions where the programming logic is hard-coded in these robots. Examples of General robots (Autonomous or semi-autonomous)
Everyday use cases of these robots are the following: 1-Pick and place items 2-Loading and unloading 3-Food Packaging 4-Spot welding 5-Steel cutting 6-Assembly applications 2-Intelligent Robots Robotics’ field is an interdisciplinary field among others it integrates exceptionally well with computer science and, to be specific, with software. Here comes the “Artificial Intelligence in Robots,” where Robots defer the action to the cognitive part that mimics human intelligence. What is this cognitive part of “Intelligent Robots”? — Answer: you can give it different names, an intelligent software module, an algorithm, a machine learning model, or a computer program that makes the intelligent decision for the integrated hardware interface function, without being explicitly programmed.” Examples of Intelligent Robots (Autonomous or semi-autonomous) 1- Tesla — is an excellent example of an intelligent robot that makes dynamic decisions based on cognitive power and leverages the learning and experience infused by a software program. 2- Drones — are autonomous and AI-based robots. They can mimic human intelligence in navigating their way to the target. They can also do all kinds of things without ever being programmed to do specific static actions. 3- Alexa — is another robot that performs specific actions based on human voice commands. Alexa provides an API (Application Programming Interface) based platform to integrate with any other pluggable interface.
So far, so good! Now the fun discussion starts here : Question # 2: Do Intelligent Robots use Machine Learning as well? Answer: Yes — Intelligent Robots (above) does. Let me explain in detail …. or wait for a second please hold on to this question. Next question, please. Question # 3: What is the difference between Artificial Intelligence and Machine Learning? Answer: Have you heard someone saying, “If it is a PowerPoint slide, it is Artificial Intelligence, and if it is Python code, it is Machine learning.” Well, let me explain this now, “Artificial Intelligence is a general concept and somewhat abstract idea, whereas Machine Learning is one of the realizations.” (Other realizations are more traditional approaches and include rule-based systems, expert systems, inference engines, fuzzy logic, etc) There are many different definitions and explanations of the differences between these two terms, but there is no such hard and fast boundary around these two. The best statement I could offer is below: Machine learning is a process in which the trained model (after learning from the historical data) starts making human-like decisions. In comparison, AI is the final state when your software mimics human intelligence, including the reasoning of those decisions.
Let me throw one more sentence to help you. “Machine Learning is where you write your code to realize your dreams of AI.” And now I hope you got the answer to your previous question as well, intelligent robots use Machine Learning to leverage Artificial Intelligence that mimics human-like behavior. Question # 4: Now, what is the difference between Machine Learning & Deep Learning? Answer: Deep Learning(DL) is a specialized branch of Machine Learning (ML), where we use a concept called “Neural Networks.” Both are the same if we talk about the objective, ML is traditional, and DL is based on the latest research and is more modern. The list of other algorithms used in Machine Learning is below: · Decision Tree Learning · Bayesian Learning · Computational Learning Theory · Reinforcement Learning · Instance-Based Learning · Genetic Algorithms · Analytical Learning In Machine Learning, there is a lot of manual work involved where the ML engineer crafts the data to extract the hidden knowledgebase (called features). Whereas in Deep Learning, most of the work is done by the Neural Network automatically by the defined and configured networks of neurons. What are Neural Networks & what is Deep Learning? A neural network is a mathematical formula developed originally by psychologist Frank Rosenblatt. This formula’s motivation was based on a neural cell or “neuron” in the human brain. The original neural network was a single-layered neural called perceptron. Later research expanded the concept to multi-layered with 100s of layered deep neural networks, hence the terminology “Deep Learning”. Great!!! Big relief to explain all this so far, now let me finish this with this last question and the most “annoying :)” question my friends frequently ask me. Question # 5: What is the difference between Data Science vs Machine Learning vs Deep Learning, can you please explain? (I have to confess that I do not have a perfect answer but let me give it a try :) Answer: Data Science is an interdisciplinary field with a broader scope than Machine Learning & Deep Learning, but the end of the day belongs to the same Artificial Intelligence Family. Data Science is more focused on statistical approaches towards data and follows the software engineering principles. It involves Data Integration, Data Exploration, Data Pre-processing, Data Visualization, and end-to-end Data model development life cycle management, focusing on the business domain. It’s the choice of a Data Scientist to choose traditional Machine Learning or modern Deep learning approaches.” As of this writing, there is no such clear demarcation of a Data Scientist and Machine Learning Engineer’s roles and responsibilities. So it is hard to differentiate, and it depends on the organizations to dictate these requirements. Let me explain with the help of the following picture. As we can see, 80 % of the time, the data scientist prepares the data in the Data Science field. It covers the whole life cycle of the traditional data problems and starts with comprehensive business domain analysis and data acquisition. Later there are multiple other stages where data is explored in detail and processed to make it ready to be fed into the advanced statistical and mathematical functions, aka. Machine-learning or deep learning algorithms. (later contributes to the 20%)
Conclusion: There is a lot of media hype in portraying Artificial Intelligence as destructive or evil robots. Average readers believe that AI and Robotics are the same fields. However, it is not valid. Robotics has a significant focus on hardware, whereas Artificial Intelligence is purely based on computer software. Furthermore, Machine learning and deep learning follow similar processes and objectives to learn from the historical dataset using statistical methods and advance mathematical functions. Machine Learning is a more traditional approach, and deep learning is more advanced, leveraging a concept called neural networks. Data Science is a broader field, but all are part of the AI family. It depends on how the organization defines its roles and responsibilities of AI vs. ML vs. DL and data science engineers.




A new year has started and it’s time to dream. Dream about the future, new opportunities and innovations that will change our lives for the better. Especially in the spheres of Artificial Intelligence (AI) there is a lot of change happening, which will affect our lives. I collected some of the predictions leadings experts say about the effects of AI. Experts have mixed feelings about AI In a lengthy interview with German business newspaper Handelsblatt, historian and best-selling author Yuval Noah Harari shares his views on the future of AI. He believes that data surveillance is one of the most significant technological threats as data can be abused to manipulate citizens, and regulation is required; democracies are best equipped to handle the downsides of artificial intelligence as it’s a complex system with a wide array of voices and opinions; artificial intelligence is the most important technology of our time; either China or the United States will develop the best AI models, which will subsequently grow their power; Europe still has a chance to avoid becoming a ‘data colony’ to the AI superpowers; there will be a massive shift in the labor market requiring training, modern education, and upskilling — however, work will not disappear. Bloomberg asked Chinese experts on the next decade for the Asian powerhouse. Ren Yi believes that artificial technology will lead to greater technological unemployment and redistribution will be necessary. Due to its Socialist values and ancient society-centered philosophy, Ren Yi sees China as a better-equipped state to manage distribution than Western capitalist societies. AI and data management will be key in 2022, according to business leaders. ZDNet asked several experts on their predictions for the new year and everybody agrees on the increasing necessity to work with AI and cutting-edge data technologies. Two thinkers put it poignantly: Zakir Hussain, EY Americas IoT Leader, believes that AI services will be crucial for generating profit. Even simple AI solutions like chatbots can reduce the pressure of labor shortages and offer individualized solutions for a broad client pool. Adam Wilson, CEO of Trifacta, believes that businesses will lose out to competitors without AI and effective data engineering. The technological advances will lead to a rise of low-code or no-code solutions that could allow people without a coding background to tailor products and client solutions. 💭 In an interview with Harvard Political Review, Liz O’Sullivan, CEO of AI company Parity, explains plenty of concerns related to bias and stereotyping in AI algorithms. She believes that artificial intelligence won’t and shouldn’t take decisions on its own in high-risk scenarios. However, she points out that there is too little research yet to grasp the extent of bias in algorithms fully. While some people claim that AI is less biased than humans, there is a risk of AI reinforcing existing stereotypes. Pieter Abbeel, a professor of electrical engineering and computer science at the University of California, Berkeley, believes that computer vision will allow robots to advance drastically. Equipping robots with the sense of sight, they will be able to maneuver through semi-structured environments in contrast to the very limited ’ very limited and fixed settings. While Abbeel believes that robots will penetrate industries and everyday lives in the next five years, he calls AI Robotics a ‘quiet revolution’ as conventional media coverage focuses on a set of narrow examples. No need to panic — but time to be mindful Artificial Intelligence will be a major force of the 2020s. AI will transform white-collar jobs and opens new opportunities for businesses. Contrary to popular belief, jobs aren’t likely to disappear entirely. Yet, many challenges come with AI. Data literacy, digital ethics and a basic understanding of the functionalities of AI will be necessary for more people. If the societies will cope with the disruption is up to the citizens, businesses and governments.






AI research keeps reaching new milestones. The deep learning paradigm keeps reaffirming its dominance year after year. It seems safe to assume that neural networks will govern the future of AI. Yet, promises keep falling short. Artificial general intelligence (AGI) doesn’t seem to be near despite what some claim. AI systems are still very dumb and narrow. There seems to be a missing gap between where AI is and where we want it to be. Are today’s approaches the way to find and close this gap? In this article, I explain why AI systems will need to be embodied, grow up, and live in the world to one day reach intelligence. Enjoy! Does the mind need a body? It was René Descartes in his 1641 book Meditations on First Philosophy, who first proposed the idea that the mind and the body are separate substances — what’s called Cartesian dualism or “disembodied intelligence.” Descartes identified the mind with our conscious subjective experience of the world and with the source of our intelligence. He thought that although we didn’t need a body to be intelligent, our mind and body interacted; physical events caused mental events. Forwarding 300 years we find a very similar idea: the mind is “a computational system that’s physically implemented by neural activity in the brain.” The computational mind was first suggested by Warren McCulloch and Walter Pitts in 1943. Jerry Fodor and Hilary Putnam extended this idea in the following decades into what’s called today the computational theory of the mind. Building upon it, in 1976 Allen Newell and Herbert A. Simon proposed the physical symbol system hypothesis (PSS hypothesis). It states that “a physical symbol system has the necessary and sufficient means for general intelligent action.” With this hypothesis, we close the circle going back to Descartes, who defended, as Emilia Bratu puts it, that “human understanding is all about forming and manipulating symbolic representations.” Bringing these ideas together we have: The body and the mind are separated. The mind is realized by computational brain activity. Intelligence appears through symbol manipulation. It was under this framework that AI first appeared after John MacCarthy proposed it as a field of research on its own in 1956. The reigning AI paradigm is bodiless Throughout the last 60 years of AI development, symbolic AI (expert systems) and connectionist AI (neural networks), have indisputably dominated the landscape. First, symbolic AI reigned during the 50s–80s period. Then, with the advent of machine learning and deep learning, neural networks took off to the status they enjoy today. Although very different in appearance, both approaches have one important thing in common: They live within the boundaries of Cartesian dualism, the computational theory of mind, and the PSS hypothesis. And hence, both reject the idea that truly intelligent AI needs a body. Today’s AI, best depicted by deep learning, has forgotten about the body. The most important advances of the decade rely on software-based AI. Systems that can generate text at the human level, beat world champions at chess or Go, test human creativity emulating Shakespeare or Bach, or will drive cars in the future. All are systems that live in the virtual world of a bodiless computer. Most interest and effort are captured by the idea that artificial general intelligence (AGI) can be realized in software-based systems in a computer. However, there are important limitations to this approach. Disembodied machines can’t acquire ‘know-how’ knowledge It was philosopher Hubert Dreyfus who first attacked the notions behind the PSS hypothesis. In his 1972 book What Computers Can’t Do, he highlighted a key difference between human intelligence and early, symbolic AI. He argued that a good part of human knowledge is tacit knowledge — know-how experiential knowledge, such as riding a bike or learning a language — , which can’t be adequately transmitted, let alone formalized or codified. Expertise is mostly tacit, Dreyfus stated, and therefore “expert” AI systems could never be truly expert. In the words of Michael Polanyi, “we can know more than we can tell.” With the advent of connectionist AI and the boom of neural networks, Dreyfus’ arguments apparently became obsolete. Machine learning systems could learn without being explicitly told what or how to learn. Face recognition, which is a great example of tacit knowledge, can be done by these systems. We can recognize the face of our mother among a thousand faces, but we don’t know how we do it. We can’t transmit the how-to knowledge and yet, machine learning systems can recognize faces even better than we do. However, Ragnar Fjelland, in a defense of Dreyfus’ arguments, stated that not even connectionist AI systems can get actual tacit knowledge. He explains that experiencing the real world is necessary to gain this type of knowledge. AI systems, in contrast, only experience — at best —the oversimplified models of reality we feed them. Machines can achieve expertise within the boundaries of a virtual world, but not more than that. In the words of Fjelland: “As long as computers do not grow up, belong to a culture, and act in the world, they will never acquire human-like intelligence.” The importance of experiencing the world We develop our understanding of the world by interacting with our surroundings. An apple isn’t just the green or red light it emits and the smooth tactile and sweet gustatory sensations. We know that an apple costs money. We know it rots eventually if we don’t eat it. We know it hurts if it falls from a tree and hits us in the head, even if it hasn’t ever happened. We understand what an apple is in all its forms because we can link information to meaning. An AI system can classify apples but it can’t understand why someone would rather eat chocolate. Because AI systems don’t live in the world, they can’t interact with it and therefore they can’t understand it. Professor of Bioengineering at the University of Genoa, Giulio Sandini argues that “to develop something like human intelligence in a machine, the machine has to be able to acquire its own experiences.” Dreyfus argued that our intelligence derives from the complex relationship between the sensory information we actively perceive and our actions on the world. We don’t passively absorb the world, as AI systems do, we “enact our perceptual experience.” Alva Noë says it best in his book Action in Perception, “perception is not a process in the brain, but a kind of skillful activity of the body as a whole. […] The world is not given to consciousness all at once but is gained gradually by active inquiry and exploration.” In sum, We are intelligent because we experience the world. Cognition and perception are active processes tied to action. We perceive the world through our bodies. Experiencing the world gives us access to tacit knowledge which leads to expertise, a hallmark of human intelligence. It seems reasonable to assume that machines will need to experience the world to be truly intelligent. The obvious question is: How could we create machines that fulfill these requirements? The promise of developmental robotics This recent field of research combines ideas from robotics, artificial intelligence, developmental psychology, and neuroscience. Scholarpedia defines its primary goal as modeling “the development of increasingly complex cognitive processes in natural and artificial systems and to understand how such processes emerge through physical and social interaction.” Developmental robotics merges robotics and AI but differs from both in two aspects. First, it emphasizes the role of the body and the environment as causal elements giving emergence to cognition. Second, artificial cognitive systems aren’t programmed. They emerge from the initiation and maintenance of a developmental process in which they interact with physical — inanimate objects — and social environments — people or other robots. Researchers use robots to test their cognitive models because they can interact with the world. Within this paradigm, developmental roboticists could eventually create a robot that grows up in the world like a human child. Alan Turing already argued in 1950 that building a child’s brain and educating it could be a better approach to create artificial intelligence than building an adult’s brain. It makes sense to follow this path towards AGI because development is the only process we know by which organisms acquire intelligence. It might not be necessary (as connectionists and symbolicists would defend), but it’s reasonable to assume that it’s “mechanistically crucial” to emulate human-like intelligence in machines. By giving AI cognitive systems a body that can develop and interact with the physical and social worlds we are merging the efforts of traditional AI with the only known instances of true intelligence. It’s at the intersection of AI, robotics, and cognitive sciences that we’ll find the path towards AGI. TL;DR Descartes popularized a school of thought on philosophy of mind that has extended its influence to this day. Disembodied approaches to AI have found significant success in the last 60 years, but they’re still way far from achieving human-like intelligence. Developmental robotics could be the answer to the remaining questions. No one can claim to have found the missing link between today’s AI and AGI, but merging AI, robotics, and the cognitive sciences could bring us closer to the only instance we have of true intelligence: Us.








