​​The term “robot” is not easily defined, but its etymology is reasonably simple to track. It is not a very old word, having been implemented into the English language fairly recently. It dates back to the early twentieth century, when Polish playwright Karel Capek presented a unique and somewhat prophetic glimpse into the future with his groundbreaking play, “Rossum's Universal Robots.” Capek chose the word “robot” based on its Old Church Slavonic origin, “rabota” – which basically translates to “slavery.” Before becoming an established fiction writer, Karel Capek worked as a journalist. And although “Rossum's Universal Robots” was a work of speculative fiction, it serves as an apt prelude to the reality of our increasingly automated tech culture. Like the more recent series of “Terminator” films, Capek's play depicts robots as future overlords who go to war with human beings. The play emphasizes how the robots were created to serve people, but gradually adopt many of their characteristics and eventually attempt to overtake them. To the extent of imitating human likeness and capability (a subset of biorobotics, which is a field in which life is imitated through technology) this story reflects largely how robots would develop over the next century. But human society thrives on efficiency, and automation is implemented where human labor becomes too costly and inefficient to justify. Technology has been a noble servant of people in many respects over the years. And although it is inspired by nature, it ultimately seeks to improve it. Thus, the robots that we’ve designed in our likeness will surpass many of our own human limitations (as many already are). As this evolution unfolds, the idea of the robot will likely become quite abstract, which raises the question of what currently defines robots as physical beings. The following five essential qualities characterize robots as we have come to know them today. Human intelligence is derived from the elaborate and interconnected network of neurons within the human brain. These neurons form electrical connections with one another, but it remains unclear how exactly they collectively cultivate brain activity like thoughts and reasoning. Nevertheless, innovations in the realms of computation and data mining enable the development of artificially intelligent systems that reflect human intellectual capability. A robot known as Kismet (developed at the Massachusetts Institute of Technology) decentralizes its computing by separating it into different processing tiers. Higher levels of computing deal with complicated and technically advanced processes, while the lower resources are allocated to the tedious and repetitive activity. Kismet works very similarly to the human nervous system, which consists of both voluntary and involuntary functionality. Artificial intelligence can be a controversial technology, including how its terminology is applied as well as the subjective nature of AI and whether or not it could ever constitute a form of consciousness. Today, much of the modern debate on human-like AI revolves around their lack of true emotions or personality. Possibly, one of the most unique traits that characterize humanity and its evolution over animals is empathy – a powerful driver influencing many of our decisions and actions. Machines still lack a true “emotional intelligence,” and it’s probably better if they never have their own emotions– unless we want to see our Alexa refusing to work because she’s angry or sad. However, the ability of modern AI to recognize human emotion may be beneficial. Even now, AI seems to to show the first signs of an early empathy–in the form of an enhanced ability to recognize human facial expressions, vocal intonation, and body language, and tune their reactions accordingly. A glimmer of very rudimental empathy has been positively identified in a recent experiment led by the engineers at Columbia Engineering’s Creative Machines Lab. Although it's a bit of a stretch to define this very primitive ability to visually predict another robot's behavior as true "empathy", this one still is a very first step towards this direction. In a nutshell, a first robot had to choose his path depending on whether he was able or not to see a certain green box in his camera. The other "empathic" robot couldn't see that, yet, after 2 hours of observation, it was eventually able to predict his partner's preferred path 98% of times even without possessing any knowledge about the green box. The technology that empowers robot senses has fostered our ability to communicate electronically for many years. Electronic communication mechanisms, such as microphones and cameras, help transmit sensory data to computers within simulated nervous systems. Sense is useful, if not fundamental to robots’ interaction with live, natural environments. The human sensory system is broken down into vision, hearing, touch, smell and taste – all of which have been or are being implemented into robotic technology somehow. Vision and hearing are simulated by transmitting media to databases that compare the information to existing definitions and specifications. When a sound is heard by a robot, for example, the sound is transmitted to a database (or “lexicon”) where it is compared among similar sound waves. Self-driving vehicles are a great example of how robotic senses work. The car is stacked with sensors such as LIDAR, RADAR, video cameras, GPS, and wheel encoders that allow it to collect data from its surroundings in real time. Advanced perception algorithms will then elaborate this raw data to allow the AI to compare it against a set of pre-defined items. This way the vehicle will be able to identify and, thus, “sense” other cars, road signs, highways, pedestrians, etc. (Read also: Are These Autonomous Vehicles Ready for Our World?) Much still needs to be done before engineers will truly be able to make human-robot interactions more genuine. A particularly coveted frontier of machine perceptivity for which modern robotics is focusing all its endeavors is the ability to recognize human emotions from facial expressions. Although not yet fully employed in robotics, early emotion recognition systems are currently tested by several tech companies, including Google, Amazon and Microsoft. These not-particularly-intelligent AI-powered systems are being used for a variety of purposes, such as empowering surveillance cameras with the ability to identify suspicious people or gauge how customers respond to advertisements. Whether these techs will be used for teaching machines how to better understand humans, or just demolish our right to privacy even more, only time will tell. Dexterity refers to the functionality of limbs, appendages and extremities, as well as the general range of motor skill and physical capability of a body. In robotics, dexterity is maximized where there is a balance between sophisticated hardware and high-level programming that incorporates environmental sensing capability. Many different organizations are achieving significant milestones in robotic dexterity and physical interactivity. The United States Department of Defense is host to the Defense Advanced Research Projects Agency (DARPA), which sponsors a great deal of innovation in the development of prosthetic limbs. This technology lends a great deal of insight into the future of robot dexterity, but not all robots imitate the human physical form (those that do are often referred to as “androids,” whose Greek etymological origin basically translates as “likeness to man”). Organizations like Boston Dynamics explore a variety of both bipedal and quadrupedal configurations (with its famous BigDog robot falling in the latter category) while expanding on the idea of extrinsic dexterity in grasping mechanisms. Anthropomorphic robotic hands that can perform delicate tasks such as opening jars or writing can be used in many circumstances where it is too dangerous for a human to use their own limbs, such as in extreme environments or when handling harmful substances and materials. Reinforcement learning (a relatively new form of machine learning), has driven forward robot dexterity. The algorithms help the machine understand which techniques are more effective in manipulating a certain object or achieving a specific task, similarly to what happens with muscle memory in animals. The results are outstandingly dexterous robots that are nearly able to emulate the level of precision of true human hands. Robots require an energy source, and there are many factors that go into deciding which form of power provides the most freedom and capability for a robotic body. There are many different ways to generate, transmit and store power. Generators, batteries and fuel cells give power that is locally stored but also temporary, while tethering to a power source naturally limits the device’s freedom and range of functions. One very notable exception would be the simple machine-based bipedal walking system that relies only on gravity to propel its walk cycle (developed at Japan’s Nagoya Institute of Technology). While this may not qualify as a stand-alone (no pun intended) robot, it could lead to innovations on how robot power could potentially be optimized, or possibly even generated. A fantastically ingenuous example of how advanced robotics power can be arranged by for soft and flexible intelligent robots is using soft smart materials such as dielectric elastomers which can be used as transducers to design intelligent wearable robotics. A wearable actuator-generator such as robotic clothing could, for example, accumulate energy from the body movements while the robot walks down a flight of stairs, only to return this stored energy to provide added power when they must climb up again those same stairs. The strain-responsive properties of these soft materials are employed to create advanced assisting robots that are nearly self-sufficient in terms of power consumption. Intelligence, sense, dexterity and power all converge to enable independence, which in turn could theoretically lead to a nearly personified individualization of robotic bodies. From its origin within a work of speculative fiction, the word “robot” has almost universally referred to artificially intelligent machinery with a certain degree of humanity to its design and concept (however distant). This automatically imbues robots with a sense of personhood. It also raises many potential questions as to whether or not a machine can ever really “awaken” and become conscious (sentient), and by extension treated as an individual subject, or "person." Modern robots have already overcome many of the hardest challenges they faced up until just a few years ago. The robot race is running at an amazingly fast pace, and we can only wonder what machines could achieve in the upcoming future.


Even the simplest human tasks are unbelievably complex. The way we perceive and interact with the world requires a lifetime of accumulated experience and context. For example, if a person tells you, “I am running out of time,” you don’t immediately worry they are jogging on a street where the space-time continuum ceases to exist. You understand that they’re probably coming up against a deadline. And if they hurriedly walk toward a closed door, you don’t brace for a collision, because you trust this person can open the door, whether by turning a knob or pulling a handle. A robot doesn’t innately have that understanding. And that’s the inherent challenge of programming helpful robots that can interact with humans. We know it as “Moravec's paradox” — the idea that in robotics, it’s the easiest things that are the most difficult to program a robot to do. This is because we’ve had all of human evolution to master our basic motor skills, but relatively speaking, humans have only just learned algebra. In other words, there’s a genius to human beings — from understanding idioms to manipulating our physical environments — where it seems like we just “get it.” The same can’t be said for robots. Today, robots by and large exist in industrial environments, and are painstakingly coded for narrow tasks. This makes it impossible for them to adapt to the unpredictability of the real world. That’s why Google Research and Everyday Robots are working together to combine the best of language models with robot learning. Called PaLM-SayCan, this joint research uses PaLM — or Pathways Language Model — in a robot learning model running on an Everyday Robots helper robot. This effort is the first implementation that uses a large-scale language model to plan for a real robot. It not only makes it possible for people to communicate with helper robots via text or speech, but also improves the robot’s overall performance and ability to execute more complex and abstract tasks by tapping into the world knowledge encoded in the language model. Complexity exists in both language and the environments around us. That’s why grounding artificial intelligence in the real world is a critical part of what we do in Google Research. A language model may suggest something that appears reasonable and helpful, but may not be safe or realistic in a given setting. Robots, on the other hand, have been trained to know what is possible given the environment. By fusing language and robotic knowledge, we’re able to improve the overall performance of a robotic system. Here’s how this works in PaLM-SayCan: PaLM suggests possible approaches to the task based on language understanding, and the robot models do the same based on the feasible skill set. The combined system then cross-references the two to help identify more helpful and achievable approaches for the robot. For example, if you ask the language model, “I spilled my drink, can you help?,” it may suggest you try using a vacuum. This seems like a perfectly reasonable way to clean up a mess, but generally, it’s probably not a good idea to use a vacuum on a liquid spill. And if the robot can’t pick up a vacuum or operate it, it’s not a particularly helpful way to approach the task. Together, the two may instead be able to realize “bring a sponge” is both possible and more helpful. We take a responsible approach to this research and follow Google’s AI’s Principles in the development of our robots. Safety is our number-one priority and especially important for a learning robot: It may act clumsily while exploring, but it should always be safe. We follow all the tried and true principles of robot safety, including risk assessments, physical controls, safety protocols and emergency stops. We also always implement multiple levels of safety such as force limitations and algorithmic protections to mitigate risky scenarios. PaLM-SayCan is constrained to commands that are safe for a robot to perform and was also developed to be highly interpretable, so we can clearly examine and learn from every decision the system makes. Whether it’s moving about busy offices — or understanding common sayings — we still have many mechanical and intelligence challenges to solve in robotics. So, for now, these robots are just getting better at grabbing snacks for Googlers in our micro-kitchens. But as we continue to uncover ways for robots to interact with our ever-changing world, we’ve found that language and robotics show enormous potential for the helpful, human-centered robots of tomorrow.



AI-enabled robots are superior to non-AI machine, as these robots are capable of performing certain tasks while adapting to their environment. Therefore, AI-empowered robots are more intelligent than other machines. One can use training to make the computer understand physical and logistical data patterns. It can act according to its environment, robots are also integrated with various functions like computer vision, motion control, and grasping objects. The first known automated type machines people understood were robots. In earlier times robots were designed with a specific task in mind. These were machines that were developed earlier without any artificial intelligence (AI) to perform only repetitive tasks. As technology has advanced, AI is being integrated into robots rapidly. This is, of course,  to develop robotics that can perform multiple tasks, and also analyze new situations with a better perception of the environment. AI-enabled robots are superior non-AI machine as these robots are capable of performing certain tasks has the ability to adapt in its environment, therefore, is more intelligent than other machines. One can use training to make the computer understand physical and logistical data patterns. It can act according to its environment, robots are also integrated with various functions like computer vision, motion control, and grasping objects. In order to understand certain situations or to locate the various objects, labeled The AI model is trained with the assistance of machine learning (ML). Image annotation (image annotation is a tool for visualizing and searching unstructured data. Enterprise-grade image annotation tools are made for fast, easy, and accurate labeling) plays a key role in creating a colossal bank full of datasets that help robots to recognize and grasp different types of objects. Machine learning is used in training an AI model, it is used to increase the level of intelligence of the AI. AI should be able to perform specific tasks or some varied actions. A set of data is used at a large scale to ensure that the AI  models can perform precisely. To achieve that, multiple datasets are fed to the ML algorithms. Practice makes perfect, the more rigorous the training, the higher is the accuracy of the AI model. Usually, AI-enabled robots are trained to recognize the objects, with the capacity to grasp the same object and the affinity to be able to move from one location to another. Machine learning is used to assist in recognizing the wide-ranging objects visible in different shapes, sizes, and various situations. The machine learning process will keep running if the robots detect new objects. This, in turn, creates a new category to detect such objects if visible again shortly. However, there are different ways of teaching a robot using machine learning. Deep learning is also an option to train such models with high-quality training data for a more accurate machine learning process. Robotics at Warehouses Warehouse requires a fair amount of manpower to manage the towering amount of inventory kept by mainly eCommerce companies to deliver the products to their customers or move from one location to another. In such scenarios, there are some robots that are trained to handle such inventories with the capability to carefully carry them from one place to another. This reduces the human workforce in performing such repetitive tasks. Robotics in Healthcare An automated solution in the medical industry and other divisions in the industry is becoming increasingly popular. AI companies are now using big data and other relevant datasets from the healthcare industry, using machine learning to creating different types of training models for different purposes. The variety ranges from medical supplies to sanitization, disinfection, and performing remote surgeries. AI in robotics is pushing machines to become more intelligent as it learns from the data and performs crucial tasks without the assistance of humans. Robotics in Agriculture Automation is assisting the farmers to improve crop yield and boost productivity. Robots play a significant role in the cultivation and harvesting. Robots have the ability to precisely detect plants, vegetables, fruits, crops, and other unwanted floras. In agriculture AI robots can execute fruit or vegetable plucking, spraying pesticides, and supervise the health conditions of plants. Robotics in Automotive The automobile industry has already reached a level of fully automated assembly lines to assemble the vehicles. Most of the major operations that handled by robotics to develop cars, in turn, reducing the cost of manufacturing. Robots are specially trained to perform certain actions with immense accuracy and efficiency. Combining AI with robotics creates machines that are more efficient with self-learning ability to recognize new objects. As of now, robotics are usually applied for industrial purposes and in various other fields to perform various actions. These actions are more accurate and are at higher efficiency than human workers. These tasks range from handling the carton boxes at warehouses to robots performing unbelievable actions to create easier means of completing the task at hand.



When people think of Artificial Intelligence (AI), the major image that pops up in their heads is that of a robot gliding around and giving mechanical replies. There are many forms of AI but humanoid robots are one of the most popular forms. They have been depicted in several Hollywood movies and if you are a fan of science fiction, you might have come across a few humanoids. One of the earliest forms of humanoids was created in 1495 by Leonardo Da Vinci. It was an armor suit and it could perform a lot of human functions such as sitting, standing and walking. It even moved as though a real human was inside it. Initially, the major aim of AI for humanoids was for research purposes. They were being used for research on how to create better prosthetics for humans. Now, humanoids are being created for several purposes that are not limited to research. Modern-day humanoids are developed to carry out different human tasks and occupy different roles in the employment sector. Some of the roles they could occupy are the role of a personal assistant, receptionist, front desk officer and so on. The process of inventing a humanoid is quite complex and a lot of work and research is put into the process. Most times, inventors and engineers face some challenges. First-grade sensors and actuators are very important and a tiny mistake could result in glitching. Humanoids move, talk and carry out actions through certain features such as sensors and actuators. People assume that humanoid robots are robots that are structurally similar to human beings. That is, they have a head, torso, arms and legs. However, this is not always the case as some humanoids do not completely resemble humans. Some are modeled after only some specific human parts such as the human head. Humanoids are usually either Androids or Gynoids. An Android is a humanoid robot designed to resemble a male human while gynoids look like female humans. Humanoids work through certain features. They have sensors that aid them in sensing their environments. Some have cameras that enable them to see clearly. Motors placed at strategic points are what guide them in moving and making gestures. These motors are usually referred to as actuators. A lot of work, finances and research are put into making these humanoid robots. The human body is studied and examined first to get a clear picture of what is about to be imitated. Then, one has to determine the task or purpose the humanoid is being created for. Humanoid robots are created for several purposes. Some are created strictly for experimental or research purposes. Others are created for entertainment purposes. Some humanoids are created to carry out specific tasks such as the tasks of a personal assistant using AI, helping out at elderly homes, and so on. The next step scientists and inventors have to take before a fully functional humanoid is ready is creating mechanisms similar to human body parts and testing them. Then, they have to go through the coding process which is one of the most vital stages in creating a humanoid. Coding is the stage whereby these inventors program the instructions and codes that would enable the humanoid to carry out its functions and give answers when asked a question. Doesn't sound so difficult, right? However, it would be foolhardy to think that creating a humanoid is as easy as creating a kite or a slingshot in your backyard. Although humanoid robots are becoming very popular, inventors face a few challenges in creating fully functional and realistic ones. Some of these challenges include: Actuators: These are the motors that help in motion and making gestures. The human body is dynamic. You can easily pick up a rock, toss it across the street, spin seven times and do the waltz. All these can happen in the space of ten to fifteen seconds. To make a humanoid robot, you need strong, efficient actuators that can imitate these actions flexibly and within the same time frame or even less. The actuators should be efficient enough to carry a wide range of actions. Sensors: These are what help the humanoids to sense their environment. Humanoids need all the human senses: touch, smell, sight, hearing and balance to function properly. The hearing sensor is important for the humanoid to hear instructions, decipher them and carry them out. The touch sensor prevents it from bumping into things and causing self-damage. The humanoid needs a sensor to balance movement and equally needs heat and pain sensors to know when it faces harm or is being damaged. Facial sensors also need to be intact for the humanoid to make facial expressions, and these sensors should be able to carry a wide range of expressions. Making sure that these sensors are available and efficient is a hard task. AI-based Interaction: The level at which humanoid robots can interact with humans is quite limited. This where Artificial Intelligence is critical. It can help decipher commands, questions, statements and might even be able to give witty, sarcastic replies and understand random, ambiguous human ramblings. However, some humanoid robots are so human-like and efficient, that they have become quite popular. Here are a few of them: Sophia: This is the world's first robot citizen. She was introduced to the United Nations on October 11, 2017. On October 25th, she was granted Saudi Arabian citizenship, making her the first humanoid robot ever to have a nationality. Sophia was created by Hanson robotics and can carry out a wide range of human actions. It is said that she is capable of making up to fifty facial expressions and can equally express feelings. She has very expressive eyes and her Artificial Intelligence revolves around human values. She has an equal sense of humor. This particular humanoid was designed to look like the late British actress, Audrey Hepburn. Since she was granted citizenship, Sophia has attended several interviews, conferences and is now one of the world's most popular humanoids. The Kodomoroid TV Presenter: This humanoid robot was invented in Japan. Her name is derived from the Japanese word for child- Kodomo- and the word 'Android'. She speaks a number of languages and is capable of reading the news and giving weather forecasts. She has been placed at the Museum of Emerging Science and Innovation in Tokyo where she currently works. Jia Jia: This humanoid robot was worked on for three years by a team at the University of Science and Technology of China before its release. She is capable of making conversations but has limited motion and stilted speech. She does not have a full range of expressions but the team of inventors plans to make further developments and infuse learning abilities in her. Although her speech and vocabulary need further work, she is still fairly realistic. Humanoid robots are here to stay and over time, with AI making progress, we might soon find them everywhere in our daily lives.




Artificial intelligence (AI) is just a supporting player in the wider world of robotics. Unlike sensationalist sci-fi flicks, robots have had an important role since the earliest days of industrialization. From manufacturing line efficiencies to support in surgery, robots are integral to many core social constructs. The never-actualized dynamic of robots with intuition could finally be on the horizon. AI, algorithms and technological applications are accelerating into new realms. With the proper implementation of ethics, diversity, and inclusive design - humans stand to benefit significantly. Tasks that can be automated are easily replaced by forays into new fields, such as alternative energy or tech. One expert puts it this way: "Security robots, including drones, will increase in use and scale within the next decade. As a result, the cost will decrease, and new capabilities will come to light. This technology offers security professionals the unique ability to enhance procedures without replacing human workers.” Each day, we are watching companies use AI and robotics to achieve good: Scientific advancements and breakthroughs Solving complex problems Generating millions of new skilled jobs in better working conditions Over the next few years, the rate of innovation and investment in AI and robotics technologies is set to surge. Far from a space-bound dream, AI and robotics are working in tandem to provide real solutions to human challenges. And, while they may require significant upfront investment, AI and robotics technologies pay for themselves many times over in terms of the productivity gains that they drive. One example of a company driving this gain is SMP Robotics. Back in 2010, when the company was founded, AI existed in very limited applications, without wide implementations throughout various industries. The team behind SMP Robotics realized that they needed a working solution for the robots to navigate autonomously, perform efficiently and become an ultimate driving force of innovations. Soon after, NVIDIA introduced their cutting-edge chipsets that provided a technological boost. Now, SMP Robotics is a leading provider of outdoor security robots and advanced monitoring solutions. Since its inception, SMP Robotics has been instrumental in using robots to improve the operational business processes of their clients. They’ve seen the following benefits time and again: Substantial increase in labor productivity Optimization of operational overhead Improvement in a level of service Utilization of advanced technological solutions Boost in productivity When companies drive increased productivity, they’re able to make better products more efficiently, and pass along this value to both consumers, employees, and shareholders. But it’s not only productivity wins that AI and robotics can drive, they’re also dramatically improving safety in our workplaces. The use of robotics in dangerous applications that will ensure humans are safer - particularly as robotics become more advanced and leverage AI technology. Unlike a human, a robot doesn’t turn up and perform slightly less than their best - it operates at 100% all the time, it is never impaired and does not take unnecessary risks Ople.ai is one company at the forefront of this risk-reducing use of AI and robotics. Their machine learning system takes in data and provides business leaders with predictive analytics. In other words, the AI technology enables robotics to determine/assess risk profiles and adjust actions accordingly. The outcome? Workers are removed from harm's way, and can be redirected and retrained to work on higher value human tasks. One key component of any AI-implementation is access. Historically, AI was relegated to the realm of people with in-depth knowledge and expertise. Ople.ai stands out as a company that is committed to the democratization of AI. If the cost and knowledge barriers of AI are removed, the possibilities are limitless. This platform went through the following steps to achieve a more useful application for AI: Use software to determine the number of AI-enabled employees. Build a model that can use data in a meaningful way (in Ople.ai’s case, to make predictions). Make the model accessible and explainable, so business leaders can make sense of the predictions and find all of the relevant and valuable information. “In general, if you can manufacture something at a smaller cost, with fewer errors, less down time and less waste (both resources used as well as extra inventory that might go bad) then you can create a tremendous shift in the profitability of manufacturers. All these can be achieved with the combination of predictive analytics and robotics.” The truth is, AI has transformed what robots are capable of doing. From streamlining operations to protecting workers, their value is immense. Innovators in this space recognize the value of robots to improve quality of life and create better, safer opportunities for workers. As Ryzhenko explains, “The future of humanity will be greatly assisted by robots and AI. The impact of such innovations will bring improvements in everyday lives and help humans to achieve their most daring dreams.”



From doing a Google search to asking Alexa to play your favorite song, it’s impossible to deny the incredible impact artificial intelligence (AI) has had on today’s world. Ironically, AI’s advances to date also shed light on its shortfalls. No matter how impressive its accomplishments have been, AI doesn’t think or understand in the same way as humans. Ask a customer service bot a question that does not fit neatly into the “script” that enables it to respond to your queries and you’ll see what I mean. That’s because today’s AI relies on analyzing massive data sets and looking for patterns and correlations. To evolve to its next phase—artificial general intelligence (AGI)—AI must be able to understand or learn the same intellectual tasks that a 3-year-old child takes for granted. As I discussed in my previous article, a 3-year-old playing with blocks understands space, namely that the blocks are physical things that exist in a three-dimensional world controlled by physical laws. The child understands the passage of time and causality: The blocks must be stacked up before they can be knocked down. The fundamental question that this example raises is, “Could you understand space, time and causality without ever having experienced them?” These are all components of what we call “common sense” and the area of intelligence where AI is currently lacking. Children learn everything by interacting with their environment. A child moves about, grasps objects, examines them from all sides and tries out actions on objects to see what happens. Playing with blocks lets the child learn that most objects are solid; square objects can be stacked, but round ones can’t; and careful stacking can build a taller stack, while careless stacking can cause a collapse. To allow AI to explore and experiment with real objects in the same way a child does—and in doing so, approach true understanding—its computational system must be integrated with a robot. That doesn’t mean it needs to be inside a robot. The AI could reside on a supercomputer with a wireless connection to the robot. Regardless, the key is for the robot to provide mobility, senses (touch, vision and hearing) and interaction with physical objects that will enable the entire system to experience rapid sensory feedback from each action it takes. This, in turn, will enable the system to begin to learn and understand, and in doing so, approach true AGI. This is a significant shift in focus. Robots are typically described in terms of what tasks they can perform for people. In this case, the robot is described in terms of the input it can provide to the AI’s “mind.” It is more a “sensory pod” than a robot. The abilities of such an AGI pod are well within the realm of today’s robotics. In fact, a fairly simple low-cost robot should be sufficient for creating AGI. A mobile pod with vision, a manipulator and touch sensors, for example, may be sufficient for the AGI to learn about the real world. The key to AGI, though, is in the robot’s “mind,” which must be able to control the robot so that it can explore its environment, try out actions and see what happens—just like a child. If this simple robot was being controlled by a potential AGI, it could learn more about dogs, say, in a few moments of interaction than an AI can learn from thousands of images of dogs contained in its data center. While robots provide the most likely path to AGI, they aren’t strictly necessary. A simulated robot in a simulated environment might be able to learn the same things as a physical robot in a real-world environment. The difficulty with this approach, however, is that progressively more accurate simulations are needed to recreate the variability and unexpectedness of the real world. It soon becomes obvious that building the simulation is more difficult than building a physical sensory pod. Once the AGI has learned about the world, the pod can be removed. Consider that if you put on a blindfold, you don’t immediately lose your ability to visualize. Your fundamental understanding of what it is to see things or know about them in the real world still exists. Further, an AGI offers the capability to copy its content to another AGI—a clone, if you will. It could be a system that has never been connected to a robot, and this system will inherit the understanding of the real world that the robotic system previously acquired. Clearly, then, understanding is simply a pattern of software and data, so there must be other pathways to creating the same result without using a robot at all. It would appear that AGI’s need for a robot is more of a practical requirement than a theoretical requirement. Solving the problems needed to create even a simple autonomous robotic system is just a quicker pathway to AGI than other approaches.



Robots have always been our point of fascination. The swiftness with which they do the work of 100 human beings keeps our eyes wide open. The Asimov robot by Honda introduced the humans of the 21st century to modern-day robotics. AI has recently seen a significant rise in its demand as well as its development in varied fields of jobs. This has led to a surge in demand for machine learning companies in India. In this blog, we will try to present the facts on the artificial intelligence vs robotics and will thus, see how come they are similar and in what sense different. It is a common perception that AI and Robotics are the same or somewhat similar. In layman’s language, AI is the brain while Robotics is the body. Robots have existed without AI in the past. And will continue to do so. As without Robots, the implementation of AI is nothing but software interaction. An artificially-intelligent robot is a term for the combination of these two technologies as it is still under research work. The augmentation of both will do wonders. But until then one needs to clear the concept that both AI and Robotics serve different purposes. This is where the need for machine learning consulting arises. Robotics is the branch of science that deals with the development of robots. Robots aim to complete the work done by human in much lesser time with better efficiency. The robots can be automatic or need some initial instructions from humans. AI is a computer science branch and it helps in developing software that can do the task that needs personal discretion, decision-making, and intelligence as these qualities cannot be otherwise programmed in a computer. AI development services can help the machine learn and perceive surroundings to adapt as per the situation. AI can even solve different problems, tackle logical reasoning and also learn languages. Aspects Robots are programmable and interact with the surrounding using sensors. They might be automatic or semi-automatic depending on the area of their application. AI is a science that depends on machine learning and algorithms. If explained in limited words, AI works on its own decision making and reasoning. Application A robot aims to simplify lifestyle and increase work productivity. A robot that can improvise the methods to work will be more than welcome. But a clear definition of the robot doesn’t imply anywhere about learning. Designing a robot undergoes a lot of physical building, external designing, and coding (or AI), which aims to enhance its decision-making capacity. But AI is all about humanizing the technological experience. AI engines found use in GPS trackers, better navigation systems, customer care chatbot, and others. There are few cases where an AI program powers robots. There’s a separate type of robot powered by AI, i.e., Artificially-Intelligent Robot. Control of Robot is through an AI program developed in a tightly knit environment. AI development companies along with Robotic development firms perform this task. AI robots have a varied spectrum of application. They found use in several departments of the same factory whereas a simple robot performs repetitive tasks with a set of programmed movements. It does not need any intelligence. Artificially Intelligent Robots There are specific examples of artificially-intelligent robots: Cobot (Non-artificially-intelligent) A Cobot can do work by itself after programming. It does not need any human help until turned off. Cobot is a collaborative robot and in this case, will do a work assigned repetitively as it is a non-AI robot. Cobot (Artificially-intelligent) The Cobot mentioned above can be further developed by the addition of AI. The addition of AI adds a perception to the robot. It also adds decision-making instincts that need a decorated algorithm. For example, a heat receptor to prevent the robot from entering furnaces while operating as the robot would be handling heat-sensitive items. Also, a camera can add a perceptional vision to the robot. This will prevent it from colliding from different elements in the factory. Software Robot A software robot is a computer program which performs a task on various software and websites by itself. They are also known as bots as they have no existence in reality and are actually computer programs. For example, a web-crawler scans the website and categorizes them for search. It might include AI engines for better performance. Software robots are not robots in real. But the work done falls in the category of robots, thereby it makes an entry in the list here. Conclusion The difference that makes the AI stand out is the ability to make decisions. It can make the software yield better results, i.e., improvisation. AI is a technological brain with wires and programming. Robots need prior instructions or codes of instruction to perform autonomously or semi-autonomously. The world looks for the amalgamation of these two principles in a much more confident manner as that might help humankind to achieve the goal untouched from centuries. Humankind has invented for the sole purpose of satisfying curiosity. Curiosity took us to the moon and is now about to take us to the mars in search of a peaceful and better habitat. This curiosity should never die as the day it will, our will to live might. Robotics is the ever-developing branch of science, from pay loading shelves to drones, the science is developing micro-drones, unmanned aeronautical vehicles (UAV) to get to the edge of the curiosity. Betterment of everything depends on the dedication of the development team, and in case of a sole leader, his motivation. The potential offered by this technology has given rise to the demand for AI consulting services around the world.




In this introduction to robotics, we take a look at what robots are, how they’re currently used, and how they might shape the world in the future. The Future of Robotics When we think of robots, we might imagine sci-fi inspired, human-like automatons. While these types of machines are still mostly still fictional, there are many other types of robots operating in the world today. But what are robots? And how will they change the world? Here, we explore the history and types of robots, some pros and cons of using them, and how they might shape the future. We’ll also outline some of the skills you’ll need to get started with robotics and highlight some courses to help you build your skills.  What are robots? Let’s start with some definitions. Most of us are familiar with the concept of robots but may struggle to actually define them as a separate entity from other types of machines. As we explore in our open step on the nature of robots, robots are different from other machines because of how they interact with the world. They can make changes to their surroundings based on their actions and respond to the world around them. Robotic systems can be defined as interconnected, interactive, cognitive and physical tools that are able to perceive the environment using sensors, reason about events, make plans using algorithms implemented in computer programs, and perform actions enabled by actuators’. Robots are tools that can autonomously sense, reason, plan, and action. As well as performing tasks independently, they can also extend human capabilities, and mimic human actions. Ultimately, the word robot is derived from the Czech word robota, meaning forced labour. What is robotics? Robotics is the discipline of creating robots. It’s a multidisciplinary field where computer science, engineering, and technology all meet. Those working in robotics focus on the design, construction, operation, and use of robots in a host of different settings. Traditionally, the field of robotics centres on creating robots to perform simple or repetitive tasks at scale or to function in hazardous conditions where humans would otherwise be unable to work. However, recent developments in machine learning and artificial intelligence means that we may see an increase in human-to-robot interactions in the future. The robotics industry is expected to grow significantly over the coming years. Estimates suggest that the sector could be worth as much as $260 billion by 2030. Much of this growth will come from professional services robots that perform useful tasks for humans, such as cleaning, delivering, and transporting. For those looking to get a more thorough introduction to robotics, our online course from the University of Reading explores the history, anatomy and intelligence of robots. Types of robots Although the concept of robots has existed for many years, it’s only been in the last few decades that they’ve grown in complexity and use. Nowadays, there are many practical applications for robots across a wide range of fields. As discussed in our open step on the applications of robots, some of these types of robots include: Industrial. Perhaps the most common use of robots is for simple and repetitive industrial tasks. Examples include assembly line processes, picking and packing, welding, and similar functions. They offer reliability, accuracy, and speed. Military. More recent developments mean that military forces worldwide use robots in areas such as UAVs (Unmanned Aerial Vehicle), UGVs (Unmanned Ground Vehicle), triage and surveillance. Service. One of the main growth areas in robotics is in the personal service industry. Uses include for manual tasks such as dispensing food and cleaning. Exploration. We often use robots to reach hostile or otherwise inaccessible areas. A good example of exploratory robots is in space exploration, such as the Curiosity Rover on Mars. Hazardous environments. Again, certain environments can be dangerous for humans to enter, such as disaster areas, places with high radiation, and extreme environments. Medical. In the world of healthcare, medtech robots are being used in all kinds of ways. Whether it’s managing laboratory specimens or assisting with surgery, rehabilitation, or physiotherapy. Entertainment. Increasingly (particularly during the pandemic), people are buying robots for enjoyment. There are several popular toy robots, and there are even robot restaurants and giant robot statues. Advantages and disadvantages of robots The field of robotics offers solutions to many different problems. As we’ll see, the future of robots could change the world we live in. However, that doesn’t mean there aren’t downsides to the technology. As explored on our open step from the University of Reading, there are various pros and cons of using robots in our modern world: Advantages of robots They can offer increased productivity, efficiency, quality and consistency in certain settings. Unlike humans, robots don’t get bored. Until they wear out, they can repeat the same process continuously. They can be very accurate, even to fractions of an inch, making them particularly useful in the manufacturing of microelectronics. Robots can work in environments that are unsafe for humans, such as with dangerous chemicals or in areas of high radiation. They don’t have physical or environmental needs in the same way humans do. Some robots have sensors and actuators which are more capable than humans. Disadvantages of robots In some industries, robots are replacing human jobs, which can create economic problems. On the whole, robots can only do what they are told to do, meaning they can’t improvise (although AI and machine learning is changing this). Current robotics technology means that most machines are less dexterous than humans and can’t compete with a human’s ability to understand what they can see. Although experts are working on developing robots that can better sense the world. Robots with practical applications are generally expensive in terms of the initial cost, maintenance, the need for extra components and the need to be programmed to do the task. The future of robotics: Will robots take over the world? Robots are already all around us, whether it’s the automated machines that assemble our vehicles or the virtual assistants that use conversational interfaces to help us around the house. Yet as we’ve seen, they’re not currently suitable for all areas of life.  But will that change in the future? Despite fears of an AI takeover, where machines replace humans as the dominant intelligence on the planet, such a scenario seems unlikely. However, business network PwC predicts that up to 30% of jobs could be automated by robots by the mid-2030s. Other reports suggest that the stock of robots worldwide could reach 20 million by 2030, with automated workers taking up to 51 million jobs in the next 10 years. So, while they may not take over the world, we can expect to see more robots in our daily lives. How robots will change the world According to a report from McKinsey, automation and machines will see a shift in the way we work. They predict that across Europe, workers may need different skills to find work. Their model shows that activities that require mainly physical and manual skills will decline by 18% by 2030, while those requiring basic cognitive skills will decline by 28%. Workers will need technological skills, and there will be an even greater need for those with expertise in STEM. Similarly, many roles will require socioemotional skills, particularly in roles where robots aren’t good substitutes, such as caregiving and teaching. We may also see robots as a more integral part of our daily routine. In our homes, many simple tasks such as cooking and cleaning may be totally automated. Similarly, with robots that can use computer vision and natural language processing, we may see machines that can interact with the world more, such as self-driving cars and digital assistants. Robotics may also shape the future of medicine. Surgical robots can perform extremely precise operations, and with advances in AI, could eventually carry out surgeries independently. The ability for machines and robots to learn could give them an even more diverse range of applications. Future robots that can adapt to their surroundings, master new processes, and alter their behaviour would be suited to more complex and dynamic tasks. Ultimately, robots have the potential to enhance our lives. As well as shouldering the burden of physically demanding or repetitive tasks, they may be able to improve healthcare, make transport more efficient, and give us more freedom to pursue creative endeavours. How to get started with robotics If you’re an aspiring roboticist, there are several ways you can get started in the industry. You’ll need to work on some key skills, such as mathematics, science, programming, and problem-solving. You’ll also need to understand the basics of robotics and get some practical experience programming and building them. With our course Robotics with Raspberry Pi, you can learn how to connect motors, add sensors and write algorithms to build your very own robot buggy. You could also develop expertise in a field such as deep learning to master areas of AI and machine learning. Although humanoid robots still remain mostly in the realm of science fiction, robotic machines are all around us. These feats of engineering already help us with many areas of life, and could transform the future for us. Despite a variety of advantages and disadvantages, one thing’s for sure; those with skills in robotics will be highly sought after in the future. Whether creating, programming or maintaining robots, there will likely always be work in the field of robotics.




The “Curly” curling robots are capturing hearts around the world. A product of Korea University in Seoul and the Berlin Institute of Technology, the deep reinforcement learning powered bots slide stones along ice in a winter sport that dates to the 16th century. As much as their human-expert-bettering accuracy or technology impresses, a big part of the Curly appeal is how we see the little machines in the physical space: the determined manner in which the thrower advances in the arena, smartly raising its head-like cameras to survey the shiny white curling sheet, gently cradling and rotating a rock to begin delivery, releasing deftly at the hog line as a skip watches from the backline, with our hopes. Artificial intelligence (AI) today delivers everything from soup recipes to stock predictions, but most tech works out-of-sight. More visible are the physical robots of various shapes, sizes and functions that embody the latest AI technologies. These robots have generally been helpful, and now they are also becoming a more entertaining and enjoyable part of our lives.c Robots Take the Spotlight in Entertainment and Education The market size of physical and interactive social and entertainment robots is expected to hit US$3.7 billion by 2023, almost quadrupling the 2016 figure, as the robots’ capabilities in vision, mobility and interaction with people continue to rapidly improve. Advanced entertainment robots are much more than toys. Embedded AI technologies such as computer vision and natural language processing enable the robots to identify and track human faces, receive and recognize voice commands. They can respond and interact with behaviour like that of a house pet, or, when integrated with IoT modules and programmable systems, function as a humanoid Alexa personal assistant. Such robots are also intriguing and engaging students in educational roles. Across applications, an overarching trend is to make these front-line machines more fun for humans to deal with. Moving beyond its traditional drone business, DJI developed RoboMaster S1, a chariot-like educational robot designed to “bridge the digital world with the real one, bringing abstract theories to life through practical operations.” It will also of course show up for thrilling robot combat sessions. Embedded with automatic driving features and six programmable artificial intelligence modules, the S1 supports Scratch and Python and encourages users to write their own programs to teach the robot to automatically move and execute complex tasks. The Intelligent Controller’s CPU supports low-latency high-definition image transmission, AI computing, and programming development. To further pique young users’ interest in science, math, physics and programming, DJI runs the annual RoboMaster Robotic Competition, a tournament in its seventh year that challenges competitors’ wit, engineering and problem-solving skills. As of August, Northeastern University (东北大学) sits atop the RoboMaster 2020 leaderboard, followed by the China University of Mining and Technology. Named after the powerful “Monkey King” character in Chinese traditional fiction, the Wukong robot was designed for the purposes of play, education, social interaction, etc., the result of a 2018 collaboration between UBTech and Tencent. Wukong integrated computer vision, natural language process, and other AI technologies to enable the friendly robot to talk, see, play music, take photos, read and react with voice commands. UBTech’s latest products, like the Jimu series, and FireBot Kit of buildable and codable robots, further expand on this robot-based approach to engage kids and encourage them to learn how to do their own coding at a very young age. Ballie is a Samsung robot product unveiled at CES 2020. Touted as resembling the Star Wars BB-8 droid character, it actually looks more like a tennis ball. Ballie connects with smart home devices and enables user control functions via voice commands. And while it’s not talking to you it can roll around, thoroughly and efficiently navigating the floor. Although Ballie’s fun functionality won a lot of attention at CES, Samsung has not yet disclosed any release date or price, and the droid ball seems likely to remain an amusing concept. Not every novel smart robot prototype gets the right bounces. It’s said that emotions — more than reason — play a huge role in humans’ purchasing and lifestyle choices. That may be why the market for social and entertainment robots will see such growth. No matter what shape these robots may take, they can be expected to continue to integrate AI and IoT technologies to evolve increasingly enjoyable humanlike interaction capabilities that will enable them to move beyond the domain of kids’ toys or Roombas with names, to become our smart assistants, teachers, and even companions.
