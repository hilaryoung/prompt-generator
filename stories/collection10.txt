In recent years the mushrooming power, functionality and ubiquity of computers and the Internet have outstripped early forecasts about technology’s rate of advancement and usefulness in everyday life. Alert pundits now foresee a world saturated with powerful computer chips, which will increasingly insinuate themselves into our gadgets, dwellings, apparel and even our bodies.
Yet a closely related goal has remained stubbornly elusive. In stark contrast to the largely unanticipated explosion of computers into the mainstream, the entire endeavor of robotics has failed rather completely to live up to the predictions of the 1950s. In those days experts who were dazzled by the seemingly miraculous calculational ability of computers thought that if only the right software were written, computers could become the articial brains of sophisticated autonomous robots. Within a decade or two, they believed, such robots would be cleaning our oors, mowing our lawns and, in general, eliminating drudgery from our lives.
Obviously, it hasn’t turned out that way. It is true that industrial robots have transformed the manufacture of automobiles, among other products. But that kind of automation is a far cry from the versatile, mobile, autonomous creations that so many scientists and engineers have hoped for. In pursuit of such robots, waves of researchers have grown disheartened and scores of start-up companies have gone out of business.
It is not the mechanical “body” that is unattainable; articulated arms and other moving mechanisms adequate for manual work already exist, as the industrial robots attest. Rather it is the computer-based articial brain that is still well below the level of sophistication needed to build a humanlike robot.
Nevertheless, I am convinced that the decades-old dream of a useful, general-purpose autonomous robot will be realized in the not too distant future. By 2010 we will see mobile robots as big as people but with cognitive abilities similar in many respects to those of a lizard. The machines will be capable of carrying out simple chores, such as vacuuming, dusting, delivering packages and taking out the garbage. By 2040, I believe, we will nally achieve the original goal of robotics and a thematic mainstay of science ction: a freely moving machine with the intellectual capabilities of a human being.
Reasons for Optimism
In light of what I have just described as a history of largely unfullled goals in robotics, why do I believe that rapid progress and stunning accomplishments are in the ofng? My condence is based on recent developments in electronics and software, as well as on my own observations of robots, computers and even insects, reptiles and other living things over the past 30 years.
The single best reason for optimism is the soaring performance in recent years of mass-produced computers. Through the 1970s and 1980s, the computers readily available to robotics researchers were capable of executing about one million instructions per second (MIPS). Each of these instructions represented a very basic task, like adding two 10-digit numbers or storing the result in a specied location in memory.
In the 1990s computer power suitable for controlling a research robot shot through 10 MIPS, 100 MIPS and has lately reached 50,000 MIPS in a few high-end desktop computers with multiple processors. Apple’s MacBook laptop computer, with a retail price at the time of this writing of $1,099, achieves about 10,000 MIPS. Thus, functions far beyond the capabilities of robots in the 1970s and 1980s are now coming close to commercial viability.
For example, in October 1995 an experimental vehicle called Navlab V crossed the U.S. from Washington, D.C., to San Diego, driving itself more than 95 percent of the time. The vehicle’s self-driving and navigational system was built around a 25-MIPS laptop based on a microprocessor by Sun Microsystems. The Navlab V was built by the Robotics Institute at Carnegie Mellon University, of which I am a member. Similar robotic vehicles, built by researchers elsewhere in the U.S. and in Germany, have logged thousands of highway kilometers under all kinds of weather and driving con­ditions. Dramatic progress in this field became evident in the DARPA Grand Challenge contests held in California. In October 2005 several fully autonomous cars successfully traversed a hazard-studded 132-mile desert course, and in 2007 several successfully drove for half a day in urban traffic ­conditions.
In other experiments within the past few years, mobile robots mapped and navigated unfamiliar ofce suites, and computer vision systems located textured objects and tracked and analyzed faces in real time. Meanwhile personal com­puters became much more adept at recognizing text and speech.
Still, computers are no match today for humans in such functions as recognition and navigation. This puzzled experts for many years, because computers are far superior to us in calculation. The explanation of this apparent paradox follows from the fact that the human brain, in its entirety, is not a true programmable, general-purpose computer (what computer scientists refer to as a universal machine; almost all computers nowadays are examples of such machines).
To understand why this is requires an evolutionary perspective. To survive, our early ancestors had to do several things repeatedly and very well: locate food, escape predators, mate and protect offspring. Those tasks depended strongly on the brain’s ability to recognize and navigate. Honed by hundreds of millions of years of evolution, the brain became a kind of ultrasophisticated—but special-­purpose—computer.
The ability to do mathematical calculations, of course, was irrelevant for survival. Nevertheless, as language trans­formed human culture, at least a small part of our brains evolved into a universal machine of sorts. One of the hallmarks of such a machine is its ability to follow an arbitrary set of instructions, and with language, such instructions could be transmitted and carried out. But because we visualize numbers as complex shapes, write them down and perform other such functions, we process digits in a monumentally awkward and inefcient way. We use hundreds of billions of neurons to do in minutes what hundreds of them, specially “rewired” and arranged for calculation, could do in milliseconds.
A tiny minority of people are born with the ability to do seemingly amazing mental calculations. In absolute terms, it’s not so amazing: they calculate at a rate perhaps 100 times that of the average person. Computers, by comparison, are millions or billions of times faster.
Can Hardware Simulate Wetware?
The challenge facing roboticists is to take general-­purpose computers and program them to match the largely special-purpose human brain, with its ultraoptimized perceptual inheritance and other peculiar evolutionary traits. Today’s robot-controlling computers are much too feeble to be applied successfully in that role, but it is only a matter of time before they are up to the task.
Implicit in my assertion that computers will eventually be capable of the same kind of perception, cognition and thought as humans is the idea that a sufciently advanced and sophisticated articial system—for example, an electronic one—can be made and programmed to do the same thing as the human nervous system, including the brain. This issue is controversial in some circles right now, and there is room for brilliant people to disagree.
At the crux of the matter is the question of whether biological structure and behavior arise entirely from physical law and whether, moreover, physical law is computable—that is to say, amenable to computer simulation. My view is that there is no good scientic evidence to negate either of these propositions. On the contrary, there are compelling indications that both are true.
Molecular biology and neuroscience are steadily uncovering the physical mechanisms underlying life and mind but so far have addressed mainly the simpler mechanisms. Evidence that simple functions can be composed to produce the higher capabilities of nervous systems comes from programs that read, recognize speech, guide robot arms to assemble tight components by feel, classify chemicals by articial smell and taste, reason about abstract matters, and so on. Of course, computers and robots today fall far short of broad human or even animal competence. But that situation is understandable in light of an analysis, summarized in the next section, that concludes that today’s computers are only powerful enough to function like insect nervous systems. And, in my experience, robots do indeed perform like insects on simple tasks.
Ants, for instance, can follow scent trails but become disoriented when the trail is interrupted. Moths follow pheromone trails and also use the moon for guidance. Similarly, many commercial robots can follow guide wires installed below the surface they move over, and some orient themselves using lasers that read bar codes on walls.
If my assumption that greater computer power will eventually lead to human-level mental capabilities is true, we can expect robots to match and surpass the capacity of various animals and then nally humans as computer-processing rates rise sufciently high. If on the other hand the assumption is wrong, we will someday nd specic animal or human skills that elude implementation in robots even after they have enough computer power to match the whole brain. That would set the stage for a fascinating sci­entic challenge—to somehow isolate and identify the fundamental ability that brains have and that computers lack. But there is no evidence yet for such a missing principle.
The second proposition, that physical law is amenable to computer simulation, is increasingly beyond dispute. Scientists and engineers have already produced countless useful simulations, at various levels of abstraction and approximation, of everything from automobile crashes to the “color” forces that hold quarks and gluons together to make up protons and neutrons.
Nervous Tissue and Computation
If we accept that computers will eventually become powerful enough to simulate the mind, the question that naturally arises is: What processing rate will be necessary to yield performance on a par with the human brain? To explore this issue, I have considered the capabilities of the vertebrate retina, which is understood well enough to serve as a Rosetta stone roughly relating nervous tissue to computation. By comparing how fast the neural circuits in the retina perform image-processing operations with how many instructions per second it takes a computer to accomplish similar work, I believe it is possible to at least coarsely estimate the information-processing power of nervous tissue—and by extrapolation, that of the entire human nervous system.
The human retina is a patch of nervous tissue in the back of the eyeball half a millimeter thick and approximately two centimeters across. It consists mostly of light-sensing cells, but one tenth of a millimeter of its thickness is populated by image-processing circuitry that is capable of detecting edges (boundaries between light and dark) and motion for about a million tiny image regions. Each of these regions is associated with its own ber in the optic nerve, and each performs about 10 detections of an edge or a motion each second. The results ow deeper into the brain along the associated ber.
From long experience working on robot vision systems, I know that similar edge or motion detection, if performed by efcient software, requires the execution of at least 100 computer instructions. Therefore, to accomplish the retina’s 10 million detections per second would necessitate at least 1,000 MIPS.
The entire human brain is about 75,000 times heavier than the 0.02 gram of processing circuitry in the retina, which implies that it would take, in round numbers, 100 million MIPS (100 trillion instructions per second) to emulate the 1,500-gram human brain. Personal computers in 2008 are just about a match for the 0.1-gram brain of a guppy, but a typical PC would have to be at least 10,000 times more powerful to perform like a human brain.
Brainpower and Utility
Though dispiriting to articial-intelligence experts, the huge decit does not mean that the goal of a humanlike articial brain is unreachable. Computer power for a given price doubled each year in the 1990s, after doubling every 18 months in the 1980s and every two years before that. Prior to 1990 this progress made possible a great decrease in the cost and size of robot-controlling computers. Cost went from many millions of dollars to a few thousand, and size went from room-lling to handheld. Power, meanwhile, held steady at about 1 MIPS. Since 1990 cost and size reductions have abated, but power has risen to about 10,000 MIPS for a home computer. At the present pace, only about 20 or 30 years will be needed to close the gap. Better yet, useful robots don’t need full human-scale brainpower.
Commercial and research experiences convince me that the mental power of a guppy—about 10,000 MIPS—will sufce to guide mobile utility robots reliably through unfamiliar surroundings, suiting them for jobs in hundreds of thousands of industrial locations and eventually hundreds of millions of homes. A few machines with 10,000 MIPS are here already, but most industrial robots still use processors with less than 1,000 MIPS.
Commercial mobile robots have found few jobs. A paltry 10,000 work worldwide, and the companies that made them are struggling or defunct. (Makers of robot manipulators are not doing much better.) The largest class of commercial mobile robots, known as automatic guided vehicles (AGVs), transport materials in factories and warehouses. Most follow buried signal-emitting wires and detect end points and collisions with switches, a technique developed in the 1960s.
It costs hundreds of thousands of dollars to install guide wires under concrete oors, and the routes are then xed, making the robots economical only for large, exceptionally stable factories. Some robots made possible by the advent of microprocessors in the 1980s track softer cues, like magnets or optical patterns in tiled oors, and use ultrasonics and infrared proximity sensors to detect and negotiate their way around obstacles.
The most advanced industrial mobile robots, developed since the late 1980s, are guided by occasional navigational markers—for instance, laser-sensed bar codes—and by preexisting features such as walls, corners and doorways. The costly labor of laying guide wires is replaced by custom software that is carefully tuned for each route segment. The small companies that developed the robots discovered many industrial customers eager to automate transport, oor cleaning, security patrol and other routine jobs. Alas, most buyers lost interest as they realized that installation and route changing required time-consuming and expensive work by experienced route programmers of inconsistent availability. Technically successful, the robots zzled commercially.
In failure, however, they revealed the essentials for success. First, the physical vehicles for various jobs must be reasonably priced. Fortunately, existing AGVs, forklift trucks, oor scrubbers and other industrial machines designed for accommodating human riders or for following guide wires can be adapted for autonomy. Second, the customer should not have to call in specialists to put a robot to work or to change its routine; oor cleaning and other mundane tasks cannot bear the cost, time and uncertainty of expert installation. Third, the robots must work reliably for at least six months before encountering a problem or a situation requiring downtime for reprogramming or other alterations. Customers routinely rejected robots that after a month of awless operation wedged themselves in corners, wandered away lost, rolled over employees’ feet or fell down stairs. Six months, though, earned the machines a sick day.
Robots exist that have worked faultlessly for years, perfected by an iterative process that xes the most frequent failures, revealing successively rarer problems that are corrected in turn. Unfortunately, that kind of reliability has been achieved only for prearranged routes. An insectlike 10 MIPS is just enough to track a few handpicked landmarks on each segment of a robot’s path. Such robots are easily confused by minor surprises such as shifted bar codes or blocked corridors (not unlike ants thrown off a scent trail or a moth that has mistaken a streetlight for the moon).
A Sense of Space
Robots that chart their own routes emerged from laboratories worldwide in the mid-1990s, as microprocessors reached 100 MIPS. Most build two-dimensional maps from sonar or laser range­nder scans to locate and route themselves, and the best seem able to navigate ofce hallways for days before becoming disoriented. Of course, they still fall far short of the six-month commercial criterion. Too often different locations in the coarse maps resemble one another. Conversely, the same location, scanned at different heights, looks different, or small obstacles or awkward protrusions are overlooked. But sensors, computers and techniques are improving, and success is in sight.
My efforts are in the race. In the 1980s at Carnegie Mellon we devised a way to distill large amounts of noisy sensor data into reliable maps by accumulating statistical evidence of emptiness or occupancy in each cell of a grid representing the surroundings. The approach worked well in two dimensions and still guides many of the robots described above.
Three-dimensional maps, 1,000 times richer, promised to be much better but for years seemed computationally out of reach. In 1992 we used economies of scale and other tricks to reduce the computational costs of three-dimensional maps 100-fold. Continued research led us to found a company, Seegrid, that sold its first dozen robots by late 2007. These are load-pulling warehouse and factory “tugger” robots that, on command, autonomously follow routes learned in a single human-guided walk-through. They navigate by three-dimensionally grid-mapping their route, as seen through four wide-angle stereoscopic cameras mounted on a “head,” and require no guide wires or other navigational markers.
Robot, Version 1.0
In 2008 desktop PCs offer more than 10,000 MIPS. Seegrid tuggers, using slightly older processors doing about 5,000 MIPS, distill about one visual “glimpse” per second. A few thousand visually distinctive patches in the surroundings are selected in each glimpse, and their 3-D positions are statistically estimated. When the machine is learning a new route, these 3-D patches are merged into a chain of 3-D grid maps describing a 30-meter “tunnel” around the route. When the tugger is automatically retracing a taught path, the patches are compared with the stored grid maps. With many thousands of 3-D fuzzy patches weighed statistically by a so-called sensor model, which is trained offline using calibrated example routes, the system is remarkably tolerant of poor sight, changes in lighting, movement of objects, mechanical inaccuracies and other perturbations.
Seegrid’s computers, perception programs and end products are being rapidly improved and will gain new functionalities such as the ability to find, pick up and drop loads. The potential market for materials-handling automation is large, but most of it has been inaccessible to older approaches involving buried guide wires or other path markers, which require extensive planning and installation costs and create inflexible routes. Vision-guided robots, on the other hand, can be easily installed and rerouted.
Fast Replay
Plans are afoot to improve, extend and miniaturize our techniques so that they can be used in other applications. On the short list are consumer robot vacuum cleaners. Externally these may resemble the widely available Roomba machines from iRobot. The Roomba, however, is a simple beast that moves randomly, senses only its immediate obstacles and can get trapped in clutter. A Seegrid robot would see, explore and map its premises and would run unattended, with a cleaning schedule minimizing owner disturbances. It would remember its recharging locations, allowing for frequent recharges to run a powerful vacuum motor, and also would be able to frequently empty its dust load into a larger container.
Commercial success will provoke competition and ac­celerate investment in manufacturing, engineering and research. Vacuuming robots ought to beget smarter cleaning robots with dusting, scrubbing and picking-up arms, followed by larger multifunction utility robots with stronger, more dexterous arms and better sensors. Programs will
be written to make such machines pick up clutter, store, retrieve and deliver things, take inventory, guard homes, open doors, mow lawns, play games, and so on. New applications will expand the market and spur further advances when robots fall short in acuity, precision, strength, reach, dexterity, skill or processing power. Capability, numbers sold, engineering and manufacturing quality, and cost-effectiveness will increase in a mutually reinforcing spiral. Perhaps by 2010 the process will have produced the rst broadly competent “universal robots,” as big as people but with lizardlike 20,000-MIPS minds that can be programmed for almost any simple chore.
Like competent but instinct-ruled reptiles, rst-generation universal robots will handle only contingencies explicitly covered in their application programs. Unable to adapt to changing circumstances, they will often perform inefciently or not at all. Still, so much physical work awaits them in businesses, streets, elds and homes that robotics could begin to overtake pure information technology commercially.
A second generation of universal robot with a mouselike 100,000 MIPS will adapt as the rst generation does not and will even be trainable. Besides application programs, such robots would host a suite of software “conditioning modules” that would generate positive and negative reinforcement signals in pre­de­ned circumstances. For example, doing jobs fast and keeping its batteries charged will be positive; hitting or breaking something will be negative. There will be other ways to accomplish each stage of an application program, from the minutely specic (grasp the handle underhand or overhand) to the broadly general (work indoors or outdoors). As jobs are repeated, alternatives that result in positive reinforcement will be favored, those with negative outcomes shunned. Slowly but surely, second-generation robots will work increasingly well.
A monkeylike ve million MIPS will permit a third generation of robots to learn very quickly from mental rehearsals in simulations that model physical, cultural and psychological factors. Physical properties include shape, weight, strength, texture and appearance of things, and ways to handle them. Cultural aspects include a thing’s name, value, proper location and purpose. Psychological factors, applied to humans and robots alike, include goals, beliefs, feelings and preferences. Developing the simulators will be a huge undertaking involving thousands of programmers and experience-gathering robots. The simulation would track external events and tune its models to keep them faithful to reality. It would let a robot learn a skill by imitation and afford a kind of consciousness. Asked why there are candles on the table, a third-generation robot might consult its simulation of house, owner and self to reply that it put them there because its owner likes candlelit dinners and it likes to please its owner. Further queries would elicit more details about a simple inner mental life concerned only with concrete situations and people in its work area.
Fourth-generation universal robots with a humanlike 100 million MIPS will be able to abstract and generalize. They will result from melding powerful reasoning programs to third-generation machines. These reasoning programs will be the far more sophisticated descendants of today’s theorem provers and expert systems, which mimic human reasoning to make medical diagnoses, schedule routes, make nancial decisions, con­gure computer systems, analyze seismic data to locate oil deposits, and so on.
Properly educated, the resulting robots will become quite formidable. In fact, I am sure they will outperform us in any conceivable area of endeavor, intellectual or physical. Inevitably, such a development will lead to a fundamental restructuring of our society. Entire corporations will exist without any human employees or investors at all. Humans will play a pivotal role in formulating the intricate complex of laws that will govern corporate behavior. Ultimately, though, it is likely that our descendants will cease to work in the sense that we do now. They will probably occupy their days with a variety of social, recreational and artistic pursuits, not unlike today’s comfortable retirees or the wealthy leisure classes.
The path I’ve outlined roughly recapitulates the evolution of human intelligence—but 10 million times more rapidly. It suggests that robot intelligence will surpass our own well before 2050. In that case, mass-produced, fully educated robot scientists working diligently, cheaply, rapidly and increasingly effectively will ensure that most of what science knows in 2050 will have been discovered by our articial progeny!


A “robot” is often defined in terms of its capability—it is a machine that can carry out a complex series of actions automatically, especially one programmable by a computer. This is a useful definition that encompasses a large proportion of conventional robots of the kind you see in science-fiction films. This definition, and the weight of established cultural views of what a robot is, has an impact on our views of what a robot could be. The best indication of this can be seen by examining cultural attitudes to robots around the world. If we type in the word “robot” to the English language version of the Google search engine we obtain images that are almost exclusively humanoid, shiny, rigid in structure and almost clinical (see fig. 1a). They also include some rather dark and aggressive-looking military-type robots. These results are skewed significantly by the cultural corpus that Google uses to mine these opinions. If we undertake the same search on the Japanese language Google site (using ロボット, the Japanese word for robot) we get a different set of results, as shown in Figure 1b. These results show far more friendly and approachable robots with fewer human-like features and more cartoon, and animal, representations. The cause of this difference is historic and due to the post-war cultural entanglement of new technologies, and robotics in particular, in the Cold War. Robots became exemplars of an alien threat. In contrast Japan did not suffer these prejudices and robots were therefore seen as benign entities. The consequence of these historical and cultural differences on robotics development is profound: Western robotics is heavily entwined in military research while Eastern robotics is focused on assist, health care, and industry. This cultural background also perpetuates our biased views of what a robot should look like and how it should behave.
Now we have the opportunity to break away from these conventions. There is no need for a robot to be humanoid, to have limbs, to walk, or to talk. Rather, we can have a much wider interpretation of what a robot is. The boundaries between smart materials, artificial intelligence, embodiment, biology, and robotics are blurring. This is how robotics will really affect the human race over the next twenty to forty years. And what an impact we can expect! From robots that can monitor and repair the natural environment to nano robots to track and kill cancer, and from robots that will lead the way to planetary colonization to robot companions to keep us from loneliness in old age. There is no part of our society or life that will not be affected by future robotics. In short, they will become ubiquitous.
Nature has always found ways to exploit and adapt to differences in environmental conditions. Through evolutionary adaptation a myriad of organisms has developed that operate and thrive in diverse and often extreme conditions. For example, the tardigrade (Schokraie et al., 2012) is able to survive pressures greater than those found in the deepest oceans and in space, can withstand temperatures from 1K (-272 °C) to 420K (150 °C), and can go without food for thirty years. Organisms often operate in symbiosis with others. The average human, for example, has about 30 trillion cells, but contains about 40 trillion bacteria (Sender et al., 2016). They cover scales from the smallest free-living bacteria, pelagibacter ubique, at around 0.5µm long to the blue whale at around thirty meters long. That is a length range of 7 orders of magnitude and approximately 15 orders of magnitude in volume! What these astonishing facts show is that if nature can use the same biological building blocks (DNA, amino acids, etc.) for such an amazing range of organisms, we too can use our robotic building blocks to cover a much wider range of environments and applications than we currently do. In this way we may be able to match the ubiquity of natural organisms.
To achieve robotic ubiquity requires us not only to study and replicate the feats of nature but to go beyond them with faster (certainly faster than evolutionary timescales!) development and more general and adaptable technologies. Another way to think of future robots is as artificial organisms. Instead of a conventional robot which can be decomposed into mechanical, electrical, and computational domains, we can think of a robot in terms of its biological counterpart and having three core components: a body, a brain, and a stomach. In biological organisms, energy is converted in the stomach and distributed around the body to feed the muscles and the brain, which in turn controls the organisms. There is thus a functional equivalence between the robot organism and the natural organism: the brain is equivalent to the computer or control system; the body is equivalent to the mechanical structure of the robot; and the stomach is equivalent to the power source of the robot, be it battery, solar cell, or any other power source. The benefit of the artificial organism paradigm is that we are encouraged to exploit, and go beyond, all the characteristics of biological organisms. These embrace qualities largely unaddressed by current robotics research, including operation in varied and harsh conditions, benign environmental integration, reproduction, death, and decomposition. All of these are essential to the development of ubiquitous robotic organisms.
The realization of this goal is only achievable by concerted research in the areas of smart materials, synthetic biology, artificial intelligence, and adaptation. Here we will focus on the development of novel smart materials for robotics, but we will also see how materials development cannot occur in isolation of the other much-needed research areas.
SMART MATERIALS FOR SOFT ROBOTS
A smart material is one that exhibits some observable effect in one domain when stimulated through another domain. These cover all domains including mechanical, electrical, chemical, optical, thermal, and so on. For example, a thermochromic material exhibits a color change when heated, while an electroactive polymer generates a mechanical output (i.e., it moves) when electrically stimulated (Bar-Cohen, 2004). Smart materials can add new capabilities to robotics, and especially artificial organisms. You need a robot that can track chemicals?—you can use a smart material that changes electrical properties when exposed to the chemical. You need a robotic device that can be implanted in a person but will degrade to nothing when it has done its job of work?—you can use biodegradable, biocompatible, and selectively dissolvable polymers. The “smartness” of smart materials can even be quantified. Their IQ can be calculated by assessing their responsiveness, agility, and complexity (for example, the number of phase changes they can undergo) (Cao et al., 1999). If we combine multiple smart materials in one robot we can greatly increase the IQ of its body.
Smart materials can be hard, such as piezo materials (Curie and Curie, 1881), flexible, such as shape memory alloys (Wu and Wayman, 1987), soft, such as dielectric elastomers (Pelrine et al., 2000), and even fluidic, such as ferrofluids (Albrecht et al., 1997) and electrorheological fluids (Winslow, 1949). This shows the great facility and variety of these materials, which largely cover the same set of physical properties (stiffness, elasticity, viscosity) as biological tissue. One important point to recognize in almost all biological organisms, and certainly all animals, is their reliance on softness. No animal, large or small, insect or mammal, reptile or fish, is totally hard. Even the insects with their rigid exoskeletons are internally soft and compliant. Directly related to this is the reliance of nature on the actuation (the generation of movement and forces) of soft tissue such as muscles. The humble cockroach is an excellent example of this; although it has a very rigid and hard body, its limbs are articulated by soft muscle tissue (Jahromi and Atwood, 1969). If we look closer at the animal kingdom we see many organisms that are almost totally soft. These include worms, slugs, molluscs, cephalopods, and smaller algae such as euglena. They exploit their softness to bend, twist, and squeeze in order to change shape, hide, and to locomote. An octopus, for example, can squeeze out of a container through an aperture less than a tenth the diameter of its body (Mather, 2006). Despite their softness, they can also generate forces sufficient to crush objects and other organisms while being dextrous enough to unscrew the top of a jar (BBC, 2003). Such remarkable body deformations are made possible not only by the soft muscle tissues but also by the exploitation of hydraulic and hydrostatic principles that enable the controllable change in stiffness (Kier and Smith, 1985).
We now have ample examples in nature of what can be done with soft materials and we desire to exploit these capabilities in our robots. Let us now look at some of the technologies that have the potential to deliver this capability. State-of-the-art soft robotic technologies can be split into three groups: 1) hydraulic and pneumatic soft systems; 2) smart actuator and sensor materials; and 3) stiffness changing materials. In recent years soft robotics has come to the fore through the resurgence of fluidic drive systems combined with a greater understanding and modelling of elastomeric materials. Although great work has been done in perfecting pneumatic braided rubber actuators (Meller et al., 2014), this discrete component-based approach limits its range of application.
A better approach is shown in the pneunet class of robotic actuators (Ilievski et al., 2011) and their evolution into wearable soft devices (Polygerinos et al., 2015) and robust robots (Tolley et al., 2014). Pneunets are monolithic multichamber pneumatic structures made from silicone and polyurethane elastomers. Unfortunately hydraulic and pneumatic systems are severely limited due to their need for external pumps, air/fluid reservoirs, and valves. These add significant bulk and weight to the robot and reduce its softness. A far better approach is to work toward systems that do not rely on such bulky ancillaries. Smart materials actuators and sensors have the potential to deliver this by substituting fluidic pressure with electrical, thermal, or photonic effects. For example, electroactive polymers (EAPs) turn electrical energy into mechanical deformation. Figures 2 and 3 show two common forms of EAP: the dielectric elastomer actuator (DEA) (Pelrine et al., 2000) and the ionic polymer actuator (IPA) (Shahinpoor and Kim, 2001). The DEA is composed of a central elastomeric layer with high dielectric constant that is sandwiched between two compliant electrode layers. When a large electric field (of the order MV/m) is applied to the composite structure, opposing charges collect at the two electrodes and these are attracted by Coulomb forces, labelled σ in Figure 2. These induce Maxwell stresses in the elastomer, causing it to compress between the electrodes and to expand in the plane, labelled ε in Figure 2. Since Coulomb forces are inversely proportional to charge separation, and the electrodes expand upon actuation, resulting in a larger charge collecting area, the induced stress in the DEA actuator is proportional to the square of the electric field. This encourages us to make the elastomer layer as thin as possible. Unfortunately, a thinner elastomer layer means we need more layers to make our robot, with a consequently higher chance of manufacturing defect or electrical breakdown. Because DEAs have power density close to biological muscles (Pelrine et al., 2000), they are good candidates for development into wearable assist devices and artificial organisms.

Ionic polymer actuators, on the other hand, are smart materials that operate through a different electromechanical principle, as shown in Figure 3. The IPA is fabricated from a central ionic conductor layer, again sandwiched by two conducting electrodes, but in contrast to DEAs the electric field is much lower (kV/m) and therefore the electrodes must be more conductive. When an electric field is applied, free ions within the ionic conductor move toward the electrodes where they collect. The high concentration of ions at the electrodes causes them to expand as like-charges repel due to local Coulomb forces. If the cations (+) and ions (-) are significantly different in size and charge, there will be a mismatch in the expansion of the two electrodes and the IPA will bend. The advantage of the IPA is that it operates at much lower voltages than the DEA, but it can only generate lower forces. A more recent addition to the smart materials portfolio is the coiled nylon actuator (Haines et al., 2014). This is a thermal actuator fabricated from a single twist-insertion-buckled nylon filament. When heated, this structure contracts. Although the nylon coil actuator has the potential to deliver low-cost and reliable soft robotics, it is cursed by its thermal cycle. In common with all other thermal actuators, including shape memory alloys, it is relatively straightforward to heat the structure (and thereby cause contraction of the muscle-like filament) but it is much more challenging to reverse this and to cool the device. As a result, the cycle speed of the nylon (and SMA) actuators is slow at less than 10Hz. In contrast, DEAs and IPAs have been demonstrated at 100’s of Hz, and the DEA has been shown to even operate as a loudspeaker (Keplinger et al., 2013).
The final capability needed to realize the body of soft robotic organisms is stiffness change. Although this may be achieved through muscle activation, as in the octopus, there are a number of soft robotic technologies that can achieve stiffness modulation independent of actuation. These include shape memory polymers (SMP) and granular jamming. SMPs are polymers that undergo a controllable and reversible phase transition from a rigid, glassy state to the soft, rubber shape (Lendlein et al., 2002). They are stimulated most commonly through heat, but some SMPs transition between phases when photonically or electrically stimulated. The remarkable property of SMPs is their ability to “memorize” a programmed state. In this way an SMP robot can be made to transition between soft and hard, and when the operation is complete it can be made to automatically return to its pre-programmed shape. One exciting possibility of SMPs is to combine them with actuators that are themselves stimulated by the same energy source. For example, a thermally operated shape memory polymer can be combined with a thermal SMP to yield a complex structure that encompasses actuation, stiffness change, and memory in one unit driven solely by heat (Rossiter et al., 2014). Granular jamming, in contrast to SMP phase change, is a more mechanical mechanism (Amend et al., 2012). A compliant chamber is filled with granular materials and the stiffness of the chamber can be controlled by pumping a fluid, such as air, into and out of it. When air is evacuated from the chamber, atmospheric pressure due to the within-chamber vacuum cases the granules to compress together and become rigid. In this way a binary soft-hard stiffness changing structure can be made. Such a composite structure is very suited to wearable assist devices and exploratory robots.
ROBOTS WHERE YOU DON’T EXPECT THEM
Having touched above on the technologies that will give us a new generation of robotics, let us now examine how these robots may appear in our lives and how we will interact, and live, with them.
Smart Skins
The compliance of soft robotics makes them ideally suited for direct interaction with biological tissue. The soft-soft interactions of a soft robot and human are inherently much safer than a hard-soft interface imposed by conventional rigid robots. There has been much work on smart materials for direct skin-to-skin contact and for integration on the human skin, including electrical connections and electronic components (Kim et al., 2011). A functional soft robotic second skin can offer many advantages beyond conventional clothing. For example, it may mimic the color-changing abilities of the cephalopods (Morin et al., 2012), or it may be able to translocate fluids like the teleost fishes (Rossiter et al., 2012) and thereby regulate temperature. The natural extension of such skins lies in smart bandages to promote healing and to reduce the spread of microbial resistance bacteria by reducing the need for antibiotics. Of course, skins can substitute for clothing, but we are some way from social acceptance of second-skins as a replacement for conventional clothing. If, on the other hand, we exploit fibrous soft actuation technologies such as the nylon coil actuator and shape memory alloy-polymer composites (Rossiter et al., 2014), we can weave artificial muscles into fabric. This yields the possibility of active and reactive clothing. Such smart garments also offer a unique new facility: because the smart material is in direct contact with the skin, and it has actuation capabilities, it can directly mechanically stimulate the skin. In this way we can integrate tactile communication into clothing. The tactile communication channel has largely been left behind by the other senses. Take, for example, the modern smartphone; it has high bandwidth in both visual and auditory outputs but almost non-existent touch stimulating capabilities. With touch-enabled clothing we can generate natural “affective” senses of touch, giving us a potentially revolutionary new communication channel. Instead of a coarse vibrating motor (as used in mobile phones) we can stroke, tickle, or otherwise impart pleasant tactile feelings (Knoop and Rossiter, 2015).
Assist Devices
If the smart clothing above is able to generate larger forces it can be used not just for communication but also for physical support. For people who are frail, disabled, or elderly a future solution will be in the form of power-assist clothing that will restore mobility. Restoring mobility can have a great impact on the quality of life of the wearer and may even enable them to return to productive life, thereby helping the wider economy. The challenge with such a proposition is in the power density of the actuation technologies within the assist device. If the wearer is weak, for example because they have lost muscle mass, they will need significant extra power, but the weight of this supplementary power could be prohibitively expensive. Therefore the assist device should be as light and comfortable as possible, with actuation having a power density significantly higher than biological muscles. This is currently beyond the state-of-the-art. Ultimately wearable assist devices will make conventional assist devices redundant. Why use a wheel chair if you can walk again by wearing soft robotic Power Pants?
Medical Devices
We can extend the bio-integration as exemplified by the wearable devices described above into the body. Because soft robotics is so suitable for interaction with biological tissue it is natural to think of a device that can be implanted into the body and which can interact physically with internal structures. We can then build implantable medical devices that can restore the functionality of diseased and damaged organs and structures. Take, for example, soft tissue cancer that can affect organs ranging from the bowels and prostate to the larynx and trachea. In these diseases a typical treatment involves the surgical excision of the cancer and management of the resulting condition. A patient with laryngeal cancer may have a laryngectomy and thereafter will be unable to speak and must endure a permanent tracheostomy. By developing and implanting a soft robotic replacement organ we may restore functional capabilities and enable the patient to once again speak, swallow, cough and enjoy their lives. Such bio-integrating soft robotics is under development and expected to appear in the clinic over the next ten to fifteen years.
Biodegradable and Environmental Robots
It is natural to extend the notion of bio-integration from the domestic (human-centric) environment to the natural environment. Currently robots that operate in the natural environment are hampered by their very underlying technologies. Because the robots are made of rigid, complex, and environmentally harmful materials, they must be constantly monitored. When they reach the end of their productive lives they must be recovered and safely disposed of. If, on the other hand, we can make the robots totally environmentally benign, we can be less concerned with their recovery after failure. This is now possible through the development of biodegradable soft robotics (Rossiter et al., 2016). By exploiting smart materials that are not only environmentally safe in operation, but which safely degrade to nothing in the environment, we can realize robots that live, die, and decay without environmental damage. This changes the way we deploy robots in the environment: instead of having to track and recall a small number of environmentally damaging robots we can deploy thousands and even millions of robots, safe in the knowledge that they will degrade safely in the environment, causing no damage. A natural extension of a biodegradability robot is one that is edible. In this case an edible robot can be eaten; it will do a job of work in the body; and then will be consumed by the body. This provides a new method for the controlled, and comfortable, delivery of treatments and drugs into the body.
Intelligent Soft Robots
All of the soft actuators described above operate as transducers. That is, they convert one energy form into another. This transduction effect can often be reversed. For example, dielectric elastomers actuators can be reconfigured to become dielectric elastomer generators (Jin et al., 2011). In such a generator the soft elastomer membrane is mechanically deformed and this results in the generation of an electrical output. Now we can combine this generator effect with the wearable robotics described above. A wearable actuator-generator device may, for example, provide added power when walking up hill, and once the user has reached the top of the hill, it can generate power from body movement as the user leisurely walks down the hill. This kind of soft robotic “regenerative braking” is just one example of the potential of bidirectional energy conversion in soft robotics. In such materials we have two of the components of computation: input and output. By combining these capabilities with the strain-responsive properties inherent in the materials we can realize robots that can compute with their bodies. This is a powerful new paradigm, often described in the more general form as embodied intelligence or morphological computation (Pfeifer and Gómez, 2009). Through morphological computation we can devolve low-level control to the body of the soft robot. Do we therefore need a brain in our soft robotic organism? In many simple soft robots the brain may be redundant, with all effective computing being performed by the body itself. This further simplifies the soft robot and again adds to its potential for ubiquity.
CONCLUSIONS
In this article we have only scratched the surface of what a robot is, how it can be thought of as a soft robotic organism, and how smart materials will help realize and revolutionize future robotics. The impact on humans has been discussed, and yet the true extent of this impact is something we can only guess at. Just as the impact of the Internet and the World Wide Web were impossible to predict, we cannot imagine where future robotics will take us. Immersive virtual reality? Certainly. Replacement bodies? Likely. Complete disruption of lives and society? Quite possibly! As we walk the path of the Robotics Revolution we will look back at this decade as the one where robotics really took off, and laid the foundations for our future world.
 
What comes to mind when you hear the word “robot”? Do you picture a metallic humanoid in a spaceship in the distant future? Perhaps you imagine a dystopian future where humanity is enslaved by its robot overlords. Or maybe you think of an automobile assembly line with robot-like machines putting cars together.
Whatever you think, one thing is sure: robots are here to stay. Fortunately, it seems likely that robots will be more about doing repetitive or dangerous tasks than seizing supreme executive power. Let’s look at robotics, defining and classifying the term, figuring out the role of Artificial Intelligence in the field, the future of robotics, and how robotics will change our lives.
What Is Robotics?
Robotics is the engineering branch that deals with the conception, design, construction, operation, application, and usage of robots. Digging a little deeper, we see that robots are defined as an automatically operated machine that carries out a series of actions independently and does the work usually accomplished by a human.
Incidentally, robots don’t have to resemble humans, although some do. Look at images of automobile assembly lines for proof. Robots that appear human are typically referred to as “androids.” Although robot designers make their creations appear human so that people feel more at ease around them, it’s not always the case. Some people find robots, especially ones that resemble people, creepy.
Types of Robots
Robots are versatile machines, evidenced by their wide variety of forms and functions. Here's a list of a few kinds of robots we see today:
Healthcare: Robots in the healthcare industry do everything from assisting in surgery to physical therapy to help people walk to moving through hospitals and delivering essential supplies such as meds or linens. Healthcare robots have even contributed to the ongoing fight against the pandemic, filling and sealing testing swabs and producing respirators.
Homelife: You need look no further than a Roomba to find a robot in someone's house. But they do more now than vacuuming floors; home-based robots can mow lawns or augment tools like Alexa.
Manufacturing: The field of manufacturing was the first to adopt robots, such as the automobile assembly line machines we previously mentioned. Industrial robots handle a various tasks like arc welding, material handling, steel cutting, and food packaging.
Logistics: Everybody wants their online orders delivered on time, if not sooner. So companies employ robots to stack warehouse shelves, retrieve goods, and even conduct short-range deliveries.
Space Exploration: Mars explorers such as Sojourner and Perseverance are robots. The Hubble telescope is classified as a robot, as are deep space probes like Voyager and Cassini.
Military: Robots handle dangerous tasks, and it doesn't get any more difficult than modern warfare. Consequently, the military enjoys a diverse selection of robots equipped to address many of the riskier jobs associated with war. For example, there's the Centaur, an explosive detection/disposal robot that looks for mines and IEDs, the MUTT, which follows soldiers around and totes their gear, and SAFFiR, which fights fires that break out on naval vessels.
Entertainment: We already have toy robots, robot statues, and robot restaurants. As robots become more sophisticated, expect their entertainment value to rise accordingly.
Travel: We only need to say three words: self-driving vehicles.
 
Advantages and Disadvantages of Robots
Like any innovation today, robots have their plusses and negatives. Here’s a breakdown of the good and bad about robots and the future of robotics.
Advantages
They work in hazardous environments: Why risk human lives when you can send a robot in to do the job? Consider how preferable it is to have a robot fighting a fire or working on a nuclear reactor core.
They’re cost-effective: Robots don’t take sick days or coffee breaks, nor need perks like life insurance, paid time off, or healthcare offerings like dental and vision.
They increase productivity: Robots are wired to perform repetitive tasks ad infinitum; the human brain is not. Industries use robots to accomplish the tedious, redundant work, freeing employees to tackle more challenging tasks and even learn new skills.
They offer better quality assurance: Vigilance decrement is a lapse in concentration that hits workers who repeatedly perform the same functions. As the human’s concentration level drops, the likelihood of errors, poor results, or even accidents increases. Robots perform repetitive tasks flawlessly without having their performance slip due to boredom.
Disadvantages
They incur deep startup costs: Robot implementation is an investment risk, and it costs a lot. Although most manufacturers eventually see a recoup of their investment over the long run, it's expensive in the short term. However, this is a common obstacle in new technological implementation, like setting up a wireless network or performing cloud migration.
They might take away jobs: Yes, some people have been replaced by robots in certain situations, like assembly lines, for instance. Whenever the business sector incorporates game-changing technology, some jobs become casualties. However, this disadvantage might be overstated because robot implementation typically creates a greater demand for people to support the technology, which brings up the final disadvantage.
They require companies to hire skilled support staff: This drawback is good news for potential employees, but bad news for thrifty-minded companies. Robots require programmers, operators, and repair personnel. While job seekers may rejoice, the prospect of having to recruit professionals (and pay professional-level salaries!) may serve as an impediment to implementing robots.
The Future of Robotics: What’s the Use of AI in Robotics?
Artificial Intelligence (AI) increases human-robot interaction, collaboration opportunities, and quality. The industrial sector already has co-bots, which are robots that work alongside humans to perform testing and assembly.
Advances in AI help robots mimic human behavior more closely, which is why they were created in the first place. Robots that act and think more like people can integrate better into the workforce and bring a level of efficiency unmatched by human employees.
Robot designers use Artificial Intelligence to give their creations enhanced capabilities like:
Computer Vision: Robots can identify and recognize objects they meet, discern details, and learn how to navigate or avoid specific items.
Manipulation: AI helps robots gain the fine motor skills needed to grasp objects without destroying the item.
Motion Control and Navigation: Robots no longer need humans to guide them along paths and process flows. AI enables robots to analyze their environment and self-navigate. This capability even applies to the virtual world of software. AI helps robot software processes avoid flow bottlenecks or process exceptions.
Natural Language Processing (NLP) and Real-World Perception: Artificial Intelligence and Machine Learning (ML) help robots better understand their surroundings, recognize and identify patterns, and comprehend data. These improvements increase the robot’s autonomy and decrease reliance on human agents.
Software robots are computer programs that perform tasks without human intervention, such as web crawlers or chatbots. These robots are entirely virtual and not considered actual robots since they have no physical characteristics.
This technology shouldn't be confused with robotic software loaded into a robot and determines its programming. However, it's normal to experience overlap between the two entities since, in both cases, the software is helping the entity (robot or computer program) perform its functions independent of human interaction.
The Future of Robotics and Robots
Thanks to improved sensor technology and more remarkable advances in Machine Learning and Artificial Intelligence, robots will keep moving from mere rote machines to collaborators with cognitive functions. These advances, and other associated fields, are enjoying an upwards trajectory, and robotics will significantly benefit from these strides.
We can expect to see more significant numbers of increasingly sophisticated robots incorporated into more areas of life, working with humans. Contrary to dystopian-minded prophets of doom, these improved robots will not replace workers. Industries rise and fall, and some become obsolete in the face of new technologies, bringing new opportunities for employment and education.
That’s the case with robots. Perhaps there will be fewer human workers welding automobile frames, but there will be a greater need for skilled technicians to program, maintain, and repair the machines. In many cases, this means that employees could receive valuable in-house training and upskilling, giving them a set of skills that could apply to robot programming and maintenance and other fields and industries.
The Future of Robotics: How Robots Will Change the World
Robots will increase economic growth and productivity and create new career opportunities for many people worldwide. However, there are still warnings out there about massive job losses, forecasting losses of 20 million manufacturing jobs by 2030, or how 30% of all jobs could be automated by 2030.
But thanks to the consistent levels of precision that robots offer, we can look forward to robots handling more of the burdensome, redundant manual labor tasks, making transportation work more efficiently, improving healthcare, and freeing people to improve themselves. But, of course, time will tell how this all works out.
If you want to become part of the robot revolution (revolutionizing how we live and work, not an actual overthrow of humanity), Simplilearn has what you need to get started. The Artificial Intelligence and Machine Learning Bootcamp, delivered in partnership with IBM and Caltech, covers vital robot-related concepts such as statistics, data science with Python, Machine Learning, deep learning, NLP, and reinforcement learning.
The bootcamp covers the latest tools and technologies from the AI ecosystem, featuring masterclasses by Caltech instructors and IBM experts, including hackathons and Ask Me Anything sessions conducted by IBM.
According to Ziprecruiter, AI Engineers in the US can earn a yearly average of $164,769, and Glassdoor reports that similar positions in India pay an annual average of ₹949,364.
Visit Simplilearn today and start an exciting new career with a fantastic future!
 

